
\section{Introduction}
We live in an age where data is the new gold. From social media interactions to online shopping habits, every click, swipe, and purchase generates a wealth of information. Everything can be summarized in bits and bytes, the language of computers, making everything reducible to data. We now have AI systems like Centaur that understand human behavior better than we do ourselves, improving upon centuries of research in psychology and cognitive science, and demonstrating an unprecedented ability to predict human choices \cite{Binz2025}. In this worldview there is a potential for optimization that is unprecendented. Whole systems including human societies can be analyzed, predicted, and even manipulated based on the data they generate. This shift towards a data-centric worldview has led to the rise of what is often referred to as "Dataism."

Dataism, a term popularized by Yuval Noah Harari, refers to a concept where data is viewed as a sacred entity, a new religion, where algorithms and data processing are seen as the ultimate means to understand and control the world, as they supposedly do a better job than humans at making decisions ~\cite{Harari2017}. Indeed, algorithms are increasingly being used to make decisions that were once the domain of human judgment, from hiring practices to medical diagnoses. The tech world is moving fast, and we see some overlap with this concept. With the recent rise of AI, we are today witnessing this movement in real time. Data is the new currency, and algorithms are the new priests, interpreting this data to make decisions that affect our lives in profound ways. More and more trust is placed in these algorithms, ChatGPT being a prime example, where people even use it to write their essays, code, poetry, and even confide their most personal issues, as if it were a therapist, believing it can solve all their problems better than they can.

Connected devices that collects our data, like wearables, apps, smart home device, and even 'smart tattoos' or implants, are a small fragment of this larger market that uses data to provide advices, recommendations, and predictions about our lives. They know an amount of information about use, from locations, sleep patterns to menstrual cycles, mood and sports metrics. Information that are quite personal. These devices are often seen as harmless tools at our service. And it seems like the general population remains uaware of how those devices can have a preponderant role in this shift towards a Dataistic society.

This is part of a larger debate about the implications on how we think about privacy, security, and individual agency. As data becomes the primary means of understanding, we'll need efficient ways to process and analyse it. Thus removing all the barriers like privacy, security, and individual agency, would seem like a better and faster way to reach this goal as data would flow more efficiently, allowing for faster and more accurate decision-making. 

This paper thus explores the implications of this shift towards a Dataistic society, focusing on the role of connected devices in shaping our understanding of privacy, security, and individual agency. It examines different aspects of it, and explores what solutions we might adopt if we wish to avoid living in a dystopian society where our data is used to make decisions about us without our consent or understanding. For example, wearables could become trackers that monitor our every move, thought, emotion, interaction, chemical reaction, and conversations. What you eat, who you connect to, what career path to follow, and which partner to choose, could be determined by algorithms designed to optimize our lives for the comon good. No privacy just transparency, where humans would just be data points in a larger system where emotion would be understood and predicted. Natural selection would be replaced by data selection, where the most efficient human would be allowed to reproduce, and the rest would just be marginalized.

Of course, this is an extreme example, and it is not here to nourish fear, but to be aware of it. This is not going to come from one day to the other. It is a very slow and gradual process, that is already happening, but we do not realize it or do not really care yet. And as small change compounds, it can lead to a very different society in the future. What legacy do we want to leave for the next generations? 

This paper is thus more focused on privacy and security as I believe they are central to moving instead towards a more harmonious world where data ownership would be more decentralized and where individuals would have greater control over their data. It is going to explores new technologies and frameworks that could help us achieve this goal and how a sample of the population perceives them. The focus is also on wearables and how they could be used to enhance our lives and help society flourish. Where data would be used to improve our lives, and amplify our creativty, not to control us. A world that values human experience and meaning, and provides equitable access for a healthier, coexisting society.

\section{Literature Review}
\subsection{Ethical Responsibilities and Deontological Perspectives}
This section explores the moral duty and responsibility of corporations in handling personal data collected from wearables.
	\subsubsection{Dataism as a New Paradigm and Its Impact on Agency}

	Human agency, a term that often refers to the capacity of individuals to act independently and make their own choices, is increasingly being challenged by the rise of Dataism. This raises ethical dilemmas about how we should treat individuals in a world where data is seen as the ultimate authority.
	Dataism has quietly changed how we think about control and responsibility with our devices. 
	In this new mindset, collecting and processing data is seen as more important than what people decide for themselves. Data becomes the main way to measure value. This can be observe through our daily lives, where we are constantly asked to share personal information with no real choice if we want to the service or not. It has become normal to gather all sorts of personal data, with companies and even governments pushing for it.

	Van Dijck~\cite{VanDijck2014} observes that this normalization gradually takes away individual autonomy. 
	People are often encouraged to give up privacy for convenience, but they might not really get what they’re giving up. There’s a belief that data is always fair and true, but that’s not always right. The way data is collected and used depends on what companies or governments want, and on what the tech can actually do. Because of this, being watched or tracked is just part of life now, and it’s hard to know when you’re sharing info by choice or when it’s just happening in the background. Companies and governments say they need to watch people for safety or to make things work better, but what really happens is that people lose control and big organizations get more power.

	De Oliveira~\cite{deOliveira2022} extends this analysis to emerging technologies like smart tattoos and other wearables, which effectively turn the human body into a continuous source of data. 
	This data is not only used for health monitoring but sometimes also repurposed for commercial or even political ends, further blurring the lines between personal well-being and systemic exploitation. Neuralink is also another, maybe more common example, we can use to illustrate this point.
	Liu~\cite{Liu2025} introduces the concept of “datafied enhancement,” where digital technologies do more than just assist individuals: they create digital representations that may not accurately reflect the person. This brings up big questions about how much control people have and whether their privacy is being taken away, since it can be hard to know or manage what happens to their data. 

	From an ethics point of view, these changes go against the old  Kantian idea that people should be treated as ends in themselves, not just as a way to make money or serve a system. But in the current data economy, people are often just seen as numbers, and their freedom to choose is pushed aside so that companies or platforms can benefit from it.
	\subsubsection{Privacy Risks and Surveillance in Wearable Technologies}

	As wearables becomes more and more mainstream, we can come to think that actors in the digital ecosystem have a moral duty to protect use privacy and autonomy. This constant, often passive, collection of data means that many users remain unaware of the amount volume of personal information their devices are transmitting. What happens to those data, who gets access, how it’s used, and for what reasons. Indeed, wearables are constantly tracking metrics like heart rate, sleep, location, menstrual cycle and even mood. This data is often sent to servers run by third parties, sometimes in other countries, and users are rarely told exactly what’s happening ~\cite{Sui2023}. This opacity undermines both trust and the possibility of meaningful consent, which is a recurring challenge in the governance of wearable technologies.

	Another aspect is that data collected for one reason can easily be used for something else. For example, companies might say they’re gathering health data to help users improve their well-being, but that same data can be instead used for targeted ads or even political profiling. We might recall the cambridge analytica scandal, where personal data was used to influence political opinions and elections ~\cite{Kanakia2019}. De Oliveira~\cite{deOliveira2022} points out that once biometric data is collected, it can quickly become a commodity, traded or used in ways most people never thought off. 

	There are also technical risks to consider. Wearables can be hacked or breached, and even with security measures in place, there are still vulnerabilities, especially if users don’t have real control over their own data.

	Social and institutional trends add another layer of complexity. As wearables become more common, the idea of “normal” data sharing keeps expanding. The promise of better health or convenience can make people overlook the risks of giving up so much information. Van Dijck ~\cite{VanDijck2014} notes that data from wearables is now treated almost like currency, exchanged for access to services. But these exchanges are rarely clear, and users often don’t realize how much control they’re giving up. Over time, this can leave people feeling less in charge of their digital lives, as they become more like data sources than active participants.

	Consent is another tricky area. Indeed, many platforms rely on broad, one-time agreements that don’t really fit the ongoing, ever-changing nature of wearable data collection. What’s needed are more flexible, context-aware ways for users to decide what they’re comfortable sharing ~\cite{Riso2017}.

	In the end, the privacy and surveillance issues tied to wearables are complicated and deeply rooted in how our society now treats data. Solving these problems will take more than just technical fixes, it will require a serious look at the social, ethical, and legal rules that shape how data is collected and used. Only by combining strong security, clear data practices, and real user empowerment can we hope to reduce the risks and get the most out of wearable technology without sacrificing privacy and autonomy.
	\subsubsection{Ethical and Regulatory Frameworks for Data Governance}

	The rapid evolution of wearable technologies has exposed significant gaps in the ethical and regulatory frameworks that are supposed to protect users’ rights and interests. As the literature shows, the legal landscape has struggled to keep pace with the technical innovations and the new forms of data collection that characterize the age of Dataism. Bouderhem~\cite{Bouderhem2023} points out that regulations like the GDPR (General Data Protection Regulation), a comprehensive data protection law in the European Union, and the EU Data Act have set important standards for data protection. But their effectiveness is often weakened by a reactive posture and a fragmented application that struggles to keep pace with technological innovation. The complexity of wearable data, its sensitivity, its volume, and the diversity of actors involved, means that traditional compliance-based approaches are not enough. The literature highlights the need for more robust technical safeguards, such as end-to-end encryption and AI-based monitoring, but also for regulatory mechanisms that can adapt to the fast-changing environment of digital health. Riso et al.~\cite{Riso2017} propose a framework of six core ethical values: scientific value, user protection, agency, trustworthiness, benefit, and sustainability, that should guide the responsible sharing of personal health data. However, in practice, there are significant gaps between these ethical ideals and the way platforms actually operate. The challenges of trust, transparency, and user empowerment are discussed in detail in subsequent sections.

	In addition, collaboration between stakeholders emerges as very important in the literature. Sui et al.~\cite{Sui2023} argue that effective governance of wearable data cannot be achieved by companies or regulators acting alone. Instead, there is a need for participatory approaches that involve users, developers, researchers, and civil society organizations in the design and oversight of data systems. This collaborative model is seen as essential for building privacy safeguards into the architecture of wearable technologies from the outset, rather than treating them as an afterthought. By involving users in the design process, companies can better align their practices with ethical principles and societal expectations, reducing the risk of backlash or regulatory intervention. However, achieving this vision is far from being easy. The literature points to several challenges, including the global nature of data flows, the diversity of legal regimes, and the technical complexity of implementing user-centric controls. Bouderhem~\cite{Bouderhem2023} notes that even the most advanced regulations can be undermined by loopholes, inconsistent enforcement, or the rapid emergence of new technologies that fall outside existing legal definitions. Moreover, there is a risk that even well-intentioned regulations could unintentionally slow down innovation or create barriers to entry for smaller competitors, thereby making the market dominance of established platforms even bigger.

	In summary, the ethical and regulatory frameworks for data governance in wearable technologies face many challenges.The literature reviewed here makes it clear that adjustments to existing laws and practices will not be enough to address the challenges posed by Dataism and the centralization of data power. What is needed is a more large scale view approach that integrates strong technical safeguards, adaptive regulation, and genuine user empowerment. 
	Only by rethinking the foundations of data governance, placing ethical values and user rights at the center, can we ensure that wearable technologies serve the interests of individuals and society, rather than simply reinforcing the 
	power of corporations or data aggregators. But it is a difficult task as pointed out by Aghion and al.~\cite{Aghion2021}, who argue that the very regulations designed to protect workers can also stifle innovation and creativity, leading to a paradox where the pursuit of ethical standards can inadvertently hinder technological progress. This highlights the need for a balanced approach that recognizes the importance of both ethical imperatives and the practical realities of fostering innovation.
	\subsubsection{Technical and Practical Challenges: Quality, Interoperability, and Consent}

	Data collectors have a duty to ensure the data they collect is accurate, interoperable, and managed with meaningful user consent. Failing to address these technical challenges means failing in their moral responsibility to protect users. These challenges are persistent and complex. As wearables become more set in daily routines, the question of how to ensure data quality, interoperability, and meaningful consent becomes important to this debate.

	A first and recurring issue is data quality. Wearable devices, despite their promise of continuous and personalized health monitoring, often produce data that is inconsistent, incomplete, or biased. Canali et al.~\cite{Canali2022} highlight that sensor variability, device calibration, and environmental factors can all affect the reliability of the data collected. Inaccurate or low-quality data can lead to false positives, overestimations of health risks, or even missed diagnoses. The consequences are technical but also ethical, as users may make decisions based on flawed information, and healthcare providers may lose trust in the systems. The literature ~\cite{Canali2022} suggests that establishing local quality standards and regular validation protocols is needed, but the fragmented nature of the wearable market makes this difficult to enforce. Without standardized benchmarks, comparing data across devices or integrating it into clinical workflows remains a challenge.


	Interoperability is another challenge. The wearable tech landscape is scattered across countless proprietary systems (a way that companies lock users into their ecosystems), locked-down data formats (a way that companies restricts how data is stored and accessed), and isolated communication protocols (a way that companies limit how devices talk to each other). This fragmentation makes it difficult for users to aggregate their data or for healthcare providers to access comprehensive records. Canali et al.~\cite{Canali2022} and Bouderhem~\cite{Bouderhem2023} show that interoperability gaps not only hurt the utility of wearables but also reinforce the dominance of large platforms that can dictate the terms of data exchange. The absence of common standards also complicates regulatory oversight and limits the potential for decentralized solutions, such as personal data vaults or blockchain-based systems, to be integrated into existing infrastructures. The literature calls for coordinated efforts among manufacturers, regulators, and standards bodies to develop interoperable frameworks.

	Consent is also a practical challenge in the context of wearables, it is detailed later in the Transparency \& Informed Consent section, traditional broad consent models are inadequate for continuous data flows and shifting purposes.

	Security is another layer that cannot be ignored. Indeed, the aggregation of sensitive health data in real time creates attractive targets for malicious actors. Even when companies implement encryption and other security measures, breaches and unauthorized access remain common, as highlighted by Mone and Shakhlo~\cite{Mone2023}. The complexity of wearable ecosystems, where data may pass through multiple devices, apps, and cloud services, increases the attack surface and makes it difficult to ensure end-to-end protection. Bouderhem~\cite{Bouderhem2023} suggests that AI-based monitoring and real-time anomaly detection could offer new safeguards, but these solutions introduce their own challenges, such as algorithmic transparency and the risk of false alarms.

	The drive to collect, process, and use more data, which is part of the Dataism paradigm, creates new technical dilema that diretly impacts the ethical responsibilities of the main actors. They have a duty to ensure the data they collect is accurate, interoperable, and managed with meaningful user consent. Failing to address these technical challenges means failing in their moral responsibility to protect users. But technical and practical challenges of quality, interoperability, and consent in wearable data governance are complex and mixed with broader issues of power, trust, and user autonomy. Addressing these challenges requires technical innovation but also a commitment to transparency, standardization, and user empowerment. Only by tackling these issues ethicaly can wearable technologies fulfill their promise of improving health and well-being without sacrificing the rights and freedoms of individuals.
\subsection{Digital Sovereignty and Consumer Empowerment}
	\subsubsection{Data Ownership \& User Control}

	The debate around data ownership and user control is at the heart of the digital sovereignty discussion, especially when it comes to wearables. In the age of Dataism, where data is seen as the ultimate authority and value, the question of who actually owns and controls the data generated by our bodies is not just technical or legal, but political and even philosophical. The literature shows that, despite the promises of empowerment and autonomy, the reality for most users is still far from true ownership or meaningful control.

	One of the first things that stands out is how the concept of “ownership” is often used in a very ambiguous way. Hummel et al.~\cite{Hummel2021} show that data sovereignty can mean legal rights, technical control, or even just the ability to manage data in practice. This ambiguity directly shapes how organizations structure their platforms and how individuals experience their rights. In the context of wearables, this ambiguity is even more problematic because the data is so personal and continuous. If ownership is not clearly defined, it is easy for platforms to claim that users “own” their data while still locking them into closed ecosystems where they cannot actually do much with it.

	Safavi and Shukur~\cite{Safavi2014} try to address this by proposing a privacy framework with ten principles, including ownership, transparency, and modifiability. Their ninth principle, “Ownership,” is especially relevant here: it says that data ownership should be fully transparent and granted solely to the user. In theory, this sounds like a strong foundation for user empowerment. But in practice, as the authors themselves point out, most commercial wearables do not implement these principles. Data is collected and stored in a file format owned and controlled by a specific company (proprietary format), and users are often given only superficial access, like a dashboard or a download button, but not the ability to truly control, delete, or transfer their data as they wish. The framework also includes practical checklists, like secure communication lines and user management, but these are rarely adopted by the big players in the industry.

	This gap between theory and practice is analysed by Conradie and Nagel~\cite{Conradie2022} who argue that the lack of meaningful user control is a direct result of the dataist business models that dominate the wearable ecosystem. Data is treated as a commodity, and the incentives are structured so that companies benefit from collecting as much as possible, often at the expense of user autonomy. Once the data leaves the device, users lose almost all practical control over what happens next. The authors suggest that a more distributed model of digital sovereignty, where control is shared among users, manufacturers, and regulators, could help. But they also recognize that this would require significant changes in both technology and governance, and that current regulations and mindsets are not yet ready for such a shift.

	The real-world consequences of this situation are visible in consumer attitudes. Arbanas et al.~\cite{Arbanas2023} report that trust in companies is falling, and that users are increasingly demanding the right to see, control, and delete their data. Many feel that no matter what actions they take, companies or hackers will still be able to get to their data. This sense of disempowerment is backed by the structure of most wearable platforms, which are designed to maximize data extraction and minimize user intervention. More on consumer behavior is discussed in the section "Behavioral Science and User Decision-Making". The literature documents many cases where data collected for one purpose is repurposed for another, often without meaningful consent or oversight ~\cite{Sui2023}. This only reinforces the perception that users are not really in control, even if the marketing says otherwise.

	To sum up, user empowerment is far from being a reality in the wearable data ecosystem. The ambiguity of data ownership, the lack of meaningful control mechanisms, and the dominance of dataist business models all contribute to a situation where users are often left in the dark about what happens to their data. Achieving these ideals is much more complicated than simply declaring that “users own their data.” It requires clear definitions, strong technical and legal mechanisms, and a willingness to rethink the incentives that currently shape the wearable data ecosystem. Without these changes, the promise of user empowerment will remain largely rhetorical, and the risks of centralized data power will continue to grow.
	\subsubsection{Transparency \& Informed Consent}

	Transparency and informed consent are often presented as the core principles of any ethical data governance framework, especially in the context of wearables. Yet, as the literature shows, the reality is much more nuanced. The promise is that users will know what happens to their data and will be able to make informed choices, but the gap between what is claimed and what is actually delivered is striking, and it is precisely this gap that undermines both trust and the very idea of digital sovereignty.

	Safavi and Shukur~\cite{Safavi2014} put transparency at the center of their privacy framework, making it one of the ten core principles for wearable health devices. They argue that users should always be able to see what data is collected, how it is processed, and who can access it. Their checklist for modifiability and transparency is supposed to guarantee that users can adjust their privacy settings as their preferences evolve. On paper, this sounds like a solid foundation for user empowerment. However, the reality is that most commercial platforms only pay lip service to these principles. Interfaces are often confusing, information is buried in lengthy terms and conditions, and the actual flows of data remain opaque for the average user. The result is that transparency becomes more of a marketing slogan than a lived reality.

	The issue of informed consent is closely tied to this lack of transparency. As discussed in the section on regulatory frameworks, current laws such as HIPAA often fail to adequately protect users of wearable technologies, especially when data is handled by entities not covered by these regulations. This leaves users with little legal protection and makes truly informed consent difficult to achieve in practice.

	This disconnection between theory and practice has real psychological and behavioral consequences. Arbanas et al.~\cite{Arbanas2023} highlight that lack of transparency is a major driver of consumer distrust. Their survey data show that users want clear, enforceable rules about data use, and they are increasingly wary of companies that fail to provide them. The psychological impact is significant: when users feel informed and in control, they are more likely to engage with digital health technologies and to benefit from them. On the other hand, opacity and complexity can lead to disengagement or even outright rejection of wearable devices.

	The literature ~\cite{Safavi2014} suggests that achieving meaningful transparency and informed consent requires much more than legal compliance or technical fixes. It demands a shift in design philosophy towards user-centricity. Interfaces need to be clear and intuitive, with privacy settings that are easy to find and adjust. Communication about data practices should be ongoing, not a one-off event at the point of installation or purchase. Consent mechanisms must be granular and revocable, allowing users to change their minds as circumstances evolve. This is especially important in the context of wearables, where data flows are continuous and the purposes for which data is used can shift over time.

	One recurring problem is that traditional models of consent are simply not fit for purpose in the age of Dataism. The old approach where users agree to a broad set of terms once and for all does not reflect the dynamic and complex nature of data collection in connected devices. The literature ~\cite{Safavi2014} calls for more dynamic, context-aware consent models, where users can adjust permissions in real time and are kept informed about how their data is being used.

	It is also worth noting that transparency and informed consent are about enabling genuine agency. When users understand what is happening with their data and can make meaningful choices, they are more likely to trust the system and to participate actively in it. This, in turn, can drive innovation and improve outcomes for everyone. But as long as transparency remains superficial and consent is reduced to a formality, the promise of digital sovereignty will remain unfulfilled.

	Dataism’s normalization of data collection has eroded user agency and made privacy trade-offs routine.
	Transparency and informed consent are thus essential for digital sovereignty and consumer empowerment, but without real transparency and dynamic, revocable consent, users cannot be empowered or in control of their data. A fundamental rethinking of how companies communicate with users and design their systems needs to happen in order to move towards a model where users are truly in control of their data, rather than passive subjects in a data-driven economy.
	\subsubsection{Technical Privacy \& Security Safeguards}

	Technical privacy and security safeguards are very important for any credible approach to digital sovereignty, especially in the context of wearables. But as the literature shows, the reality is more complex than simply deploying encryption or creating technical features. How they are implemented, integrated, and how they actually empower users rather than just protecting data in the abstract is what matters.

	Abouelmehdi et al.~\cite{Abouelmehdi2018} provide a useful starting point by comparing anonymization and encryption techniques for healthcare data. Their analysis is quite comprehensive, showing none of those methods dominates the other. Anonymization, for example, is supposed to remove personal identifiers, but in practice, re-identification attacks are still possible, especially when datasets are combined. Encryption, on the other hand, is only as strong as its key management and the security of the endpoints. The authors highlight that combining both methods can significantly enhance protection, but this comes at the cost of increased complexity and computational technicity. This is particularly relevant for wearables, where devices are often resource-constrained and users expect seamless experiences.

	The Safavi and Shukur~\cite{Safavi2014} privacy framework, cited in the previous section, takes a more holistic approach. Their ten principles and nine practical checklists merge privacy and security into the design of wearable health devices from the bottom up. Principles like secure communication lines, access control, and auditability are complemented by requirements for user-friendly interfaces and transparency. What stands out is the emphasis on usability: security measures should not make the device harder to use, otherwise users will simply bypass them or abandon the product. This comes accross often in the literature, technical safeguards are only effective if they are accessible and understandable to the people they are supposed to protect.

	The challenge, however, is that those frameworks are good for academia, but the reality is that most commercial platforms do not fully implement these frameworks. There is often a gap between what is technically possible and what is actually deployed in the market. Conradie and Nagel~\cite{Conradie2022} argue that robust security is a prerequisite for digital sovereignty, but they also point out that weak protections are still common, exposing users to exploitation and undermining trust. The business incentives are not always aligned with user empowerment: companies may prioritize speed to market or cost savings over comprehensive security, especially if users are not demanding more.

	Hummel et al.~\cite{Hummel2021} add another layer by discussing technical management strategies as part of a broader approach to sovereignty. Strong cryptography, secure storage, and access controls are necessary, but not sufficient. The authors argue that technical safeguards must be integrated with legal and organizational measures, and that users need to be aware of how these systems work in practice. Otherwise, security becomes a black box, something that happens in the background, but does not actually give users more control or understanding.

	Another point that emerges from the literature is the importance of continuous adaptation. Threats evolve quickly, and what is considered secure today may be obsolete tomorrow. In the context of wearables, where new devices, apps, and integrations are constantly being released, security measures need to be updated regularly, and companies must be proactive in monitoring for vulnerabilities and responding to incidents. This is complex and requires strong technical expertise, but also organizational commitment and regulatory oversight.

	Finally, it is important to recognize that technical safeguards alone are not enough. They must be part of a broader ecosystem that includes transparent communication, user education, and responsive governance. Users need to understand what protections are in place, how to use them, and what to do if something goes wrong. Otherwise, even the best technical solutions will fail to deliver real empowerment or trust.

	Centralized model have normalized privacy trade-offs and dimnished user agengy. Technical privacy and security safeguards are essential for digital sovereignty, but they need to be complemented. Usability, education and responsive governance are also needed to move towards a model where users are not just protected, but genuinely empowered to control their data.
	\subsubsection{Regulatory Frameworks \& Accountability}

	Following from the broad overview and challenges of the section 'Ethical and Regulatory Frameworks for Data Governance', specific issue of accountability, enforcement, and the need for adaptive regulation emerges.

	Banerjee et al.~\cite{Banerjee2018} provide a critique of the current state of regulation, using the HIPAA Partnership-Identity Exposure Matrix to show how existing laws often fail to address the specific risks posed by wearables. Their analysis suggest that: the highest risks occur when identifiable health data is handled by non-covered entities, which are not subject to HIPAA’s protections. In these cases, users have little legal recourse if their data is misused or breached. The authors argue that the law has not kept pace with the realities of continuous, granular data collection and sharing. They call for legislative updates that reflect the new dynamics of wearable data, including more robust requirements for transparency and user consent.

	Conradie and Nagel~\cite{Conradie2022} go further, pointing out that the regulatory gaps are not just technical oversights but are rooted in the way power is distributed in the digital ecosystem. They argue that current frameworks often prioritize the interests of manufacturers and service providers over those of users. This is especially problematic in the context of digital sovereignty, where the goal should be to empower individuals to control their own data. The authors advocate for reforms that clarify data ownership, mandate user-centric controls, and establish clear lines of accountability. Without these, the promise of digital sovereignty risks becoming little more than a slogan.

	Hummel et al.~\cite{Hummel2021} add another layer to the discussion by highlighting the inconsistency of legal definitions across jurisdictions. Their review of over 300 publications shows that terms like “data sovereignty” and “digital sovereignty” are used in different ways depending on the legal, technical, or cultural context. This lack of harmonization creates confusion for both users and companies, especially when data crosses borders. The authors argue for more consistent, harmonized policies that support both individual rights and the realities of cross-border data flows.

	From a consumer perspective, Arbanas et al.~\cite{Arbanas2023} highlight the growing demand for stronger government oversight and corporate accountability. Their survey data show that trust in companies is falling, and that users want clear rules about data use, the right to see and delete their data, and meaningful consequences for misuse or breaches. Self-regulation, where companies set their own rules, has not been sufficient to protect users. Instead, there is a call for more robust government intervention, including independent audits, clear penalties for violations, and mechanisms for users to seek compensation when their rights are violated.

	One recurring theme is that effective regulation must be adaptive and responsive to technological change. Static laws quickly become outdated in a field where new devices, data types, and business models are constantly emerging. The literature points to the need for regulatory frameworks that are flexible, inclusive, and capable of evolving alongside technology. This includes not only updating existing laws but also creating new forms of oversight that involve users, civil society, and technical experts in the regulatory process. Participatory governance models, where stakeholders have a real voice in shaping the rules, are seen as a promising way forward.

	Another important point is the interplay between self-regulation and government mandates. While industry codes and best practices can help set standards, they are often voluntary and lack real enforcement power. The literature suggests that a combination of self-regulation and government mandates is needed, with clear lines of responsibility and accountability. This is especially important for cross-border data flows, where national laws may conflict or leave gaps that can be exploited by less scrupulous actors.

	In summary, regulation must be rethought to prioritize user rights, transparency, and adaptive governance. Regulatory frameworks are essential for enabling true digital sovereignty and user empowerment. Without effective, user-centric regulation, technical safeguards and user control mechanisms cannot deliver real protection or agency. 
\subsection{Behavioral Science and User Decision-Making}
	\subsubsection{Internal Calculus}

	In the context of Dataism, users may lean towards sharing their personal information, as algorithms are increasingly seen as capable of making better decisions than humans. Privacy concerns may thus fade away in the face of perceived benefits, such as improved health outcomes or personalized services. 

	The literature on privacy calculus shows however that the logic behind user decisions is often more complex than a simple trade-off between risk and reward. It tries to explain this by suggesting that users weigh the benefits they expect to gain, like better health insights, convenience, or even social status, against the possible downsides, such as data misuse or loss of control. But this calculation is not always rational, and differs from person to person.

	The privacy calculus is shaped by a mix of personal, social, and contextual factors. For example, Li et al.~\cite{Li2016} show that people are more likely to accept privacy risks if they believe the benefits are substantial or if they trust the brand behind the device. Indeed, trust in the provider can act as a kind of shortcut in decision-making, especially when the technical details of data handling are too complex for most users to fully understand. At the same time, the perceived usefulness of the device, whether it actually helps users achieve their health goals, plays a decisive role. If a device is seen as essential or uniquely valuable, privacy concerns tend to fade away.

	Nevertheless, the calculus is not just about individual preferences. Social influence and cultural context also matter. Pentina et al. ~\cite{Pentina2016} point out that personality traits like extraversion and agreeableness make people more likely to focus on the benefits of using mobile apps, even when those apps require access to sensitive information. In some cultures, the collective attitude towards technology and privacy can further tip the balance. For instance, in environments where sharing data is normalized or even encouraged, users may feel less hesitation, regardless of their own concerns. Indeed, this resonates with Dataism which often promotes the idea that more data leads to better outcomes, both for individuals and society as a whole, Harari ~\cite{Harari2017}.

	Another layer comes from demographic and psychological factors. ~\cite{Kordzadeh2016} found that older adults and those in better health are generally more cautious about sharing personal health information, while people with poorer health or strong emotional ties to online health communities are more willing to take risks. This suggests that the privacy calculus is not static and that it shifts depending on life circumstances and the perceived urgency of the benefits. For someone facing a serious health issue, the promise of support or improved care can outweigh even significant privacy risks.

	Yet, despite all these influences, there is a persistent gap between what users say and what they do, which is called the privacy paradox. Many users express concern about privacy but still go ahead and use devices or apps that collect large amounts of personal data. Sun et al.~\cite{Sun2015} suggest that this happens because the immediate benefits of sharing, like enjoyment or social connection, often feel more real than abstract concerns about privacy. The hedonic effect. Gender differences also appear, as women tend to be more sensitive to privacy risks, while men are more driven by perceived benefits, but both groups are influenced by the context and the type of benefit offered.

	What complicates the picture further is the general lack of awareness about the real risks involved. Studies like ~\cite{Cilliers2019} and ~\cite{Datta2018} show that many users do not fully understand how their data is collected, processed, or potentially misused. This lack of knowledge can lead to underestimating the risks and making decisions that do not truly reflect personal preferences or values. In other words, the privacy calculus is often based on incomplete or inaccurate information, which raises questions about how meaningful these trade-offs really are.

	The literature also points out that the design of consent mechanisms and privacy controls can influence the calculus. If privacy settings are buried in complex menus or written in complicated legal terms, users are more likely to accept default options without much thought ~\cite{Banerjee2018}. On the other hand, clear communication and user-friendly interfaces can empower users to make more informed choices. ~\cite{Gupta2023} demonstrate that when users are given real options, like the ability to delete data or control who sees it, they are more willing to share information, as the perceived risks are reduced.

	While the emergence of dataism has normalized those behavioral trade-offs, we understand in this section why users make those choices. Understanding these trade-offs is important for developing technologies and policies that respect user autonomy without sacrificing the potential benefits of innovation. As wearables become more integrated into daily life, the challenge will be to create environments where users can make truly informed decisions, rather than being nudged into choices that serve the interests of others.
	\subsubsection{Extermal Pressures (Trust and Transparency)}

	Trust is fundamental for the adoption and ongoing use of wearable health technologies, yet it is also very fragile. The literature consistently shows that a user’s willingness to share sensitive health data depends less on technical security or perceived usefulness, and more on their confidence in the institutions and systems that handle their information. This confidence is hard to be won, especially in an environment where the lines between commercial interests, healthcare, and personal privacy are becoming increasingly blurred.

	Healthcare professionals and public institutions are more trusted than commercial companies. Hutchings et al.~\cite{Hutchings2020} highlight that people are generally open to sharing their medical data for research or treatment if they believe the recipient is trustworthy and the purpose is clear. The situation changes  when data is handled by private companies, especially those whose business models rely on data monetization. Users become more reluctant to share, fearing misuse or loss of control. This reflects an anxiety about the commodification of personal health data, an important aspect in the age of Dataism, where the boundary between patient and product grows increasingly thin.

	Transparency influences trust. As discussed in the section about 'Transparency and Informed Consent', the gap between the promise of transparency and the reality was highlighted, where interfaces are confusing and data flows remain opaque, which undermines user trust and digital sovereignty. When users are confronted with lengthy terms and conditions or vague privacy policies, they are left with little choice but to take the decision to accept or abandon the service. This “take it or leave it” approach reinforces the power imbalance between users and companies.

	We have seen that regulations often lags behind technological innovation. Niknejad et al.~\cite{Niknejad2020} further note that the lack of long-term trust and regulatory clarity is a one of the reason why many users abandon wearables after only a few months, even if they initially perceived significant benefits.

	The literature also highlights that transparency is also about the user experience. Gupta et al.~\cite{Gupta2023} demonstrate that when users are given clear options, like explicit consent, the ability to delete data, and oversight mechanisms, their willingness to share health information increases. This suggests that trust can be built through concrete, user-centric measures rather than abstract legal policies. However, when consent processes are opaque or overly complex, users may disengage or refuse to share data, regardless of potential benefits.

	Pentina et al.~\cite{Pentina2016} and Li et al.~\cite{Li2019} interestingly shows that the social context also influence user decision making. They both show that users are influenced by the attitudes and behaviors of their peers, as well as by the perceived reputation of device manufacturers. Positive word-of-mouth and social proof can enhance trust, while negative publicity or reports of data breaches can quickly erode it. This dynamic is particularly relevant in the context of Dataism, where the normalization of data sharing can mask underlying risks and shift the boundaries of what is considered acceptable.

	Despite these insights, the regulatory landscape remains fragmented and inconsistent. Laws and standards vary widely across jurisdictions, and enforcement is often weak or reactive. This creates opportunities for less scrupulous actors to exploit loopholes or operate in legal grey zones. The literature calls for updated laws and standards that address the unique risks of wearable data, mandate transparency, and hold companies accountable for misuse or breaches. At the same time, companies must go beyond mere compliance, adopting user-centric approaches that prioritize clear communication, granular consent, and ongoing engagement with users.

	Ethical problems identified earlier have a direct influence in shaping those behavioral dynamics. Users decisions to adopt and continue using wearable technologies are influenced my a multitude of external factors, towards the normalization of data sharing, often providing little choice than go with the flow of what is currently the norm. Centralized models thus have normalized privacy trade-offs and diminished user agency, leading to a situation where users feel they have little control over their data. The mechanisms of that erosion: broken trust, lack of transparency, and inadequate regulation.
	\subsubsection{Inherent User Traits}

	We have looked at how users make decisions and the factors that influence their choice. But who are these users, and how do they differ in their values. We are going to look at demographic and psychological factors that are very important to how privacy decisions are made. They often explain the diversity of attitudes and behaviors that we see in the real world.

	Age is one of the most consistent factors shaping privacy attitudes in the context of wearable health technologies. Kim and Ho~\cite{Kim2021} show that older adults are generally more cautious and risk-averse, especially when it comes to sharing sensitive health data. For them, the perceived risks often outweigh the potential benefits, and they are more likely to scrutinize privacy policies or avoid using certain features altogether. On the other hand, younger users tend to be more open to experimentation and are often influenced by social trends or peer recommendations. They may be more willing to share data if they see a clear benefit, such as improved health insights or social connectivity, but this openness can sometimes lead to underestimating the risks involved.

	Health status is another important factor. Kordzadeh et al.~\cite{Kordzadeh2016} found that people with chronic health conditions or acute health needs are often more willing to share personal information if it means getting better care or support. For these users, the promise of tangible benefits can outweigh privacy concerns, especially if they feel emotionally connected to a virtual health community or trust the provider. However, this willingness is not universal. Healthier individuals, or those who do not see an immediate benefit, are more likely to be cautious and to question why their data is needed in the first place. This dynamic shows that privacy decisions shift depending on personal circumstances and perceived urgency.

	Gender and cultural background also play a role, though the patterns seems to be a bit less clear. Sun et al.~\cite{Sun2015} report that women are generally more sensitive to privacy risks, especially when the benefits of sharing are hedonic rather than strictly utilitarian. Men, by contrast, are often more motivated by the perceived advantages, such as convenience or fun, and may be less concerned about potential downsides. 

	Cultural is also briefly explored. In some societies, sharing health data is normalized and even encouraged, while in others, there is a stronger emphasis on individual privacy and data protection. Pentina et al.~\cite{Pentina2016} suggest that these cultural differences can shape both the adoption of wearables and the way users think about privacy, but more research is needed to fully understand these effects.

	Personality traits further complicate the picture. Extraversion and agreeableness, for example, are linked to a greater willingness to share data, as these individuals are more likely to see the social or informational benefits of wearables~\cite{Pentina2016}. In contrast, people who are more risk-averse or have higher levels of neuroticism may be reluctant to engage with new technologies, regardless of the potential rewards. This diversity of psychological profiles means that one-size-fits-all privacy solutions are unlikely to work. Instead, there is a need for adaptive frameworks that can accommodate different user preferences and risk tolerances.

	Emotional attachment and social motivations are also significant. Kordzadeh et al.~\cite{Kordzadeh2016} highlight that users who feel a strong connection to a virtual health community are more likely to share sensitive information, even if they have privacy concerns. This is closely related to the privacy paradox, where people’s actions do not always match their stated worries. Social influence, peer validation, and the desire for support can all tip the balance in favor of sharing, especially when users see others doing the same without apparent negative consequences.

	Rigid and simplisitc current state of centralized platforms are not fit for the diverse motivations and behaviors of the multitude of user group that we have seen. While we previously observed how users weigh risks and benefits in the 'Privacy Calculus' section. Here we see what makes that calculus so diverse and unique for every individual. Demographic and psychological factors adds thus a layer of nuance, showing how those user decisions are so varied and context-dependent. Recognizing this diversity is crucial for designing privacy frameworks and technologies that are both effective and respectful of individual differences. As wearables become more integrated into daily life, the challenge will be to move beyond generic solutions and towards adaptive, user-centric approaches that empower all users to make informed choices about their data. Understanding how these diverse user groups will perceive and adopt new, decentralized solutions is thus interesting and necessary for the future of wearables and data governance.
	\subsubsection{Privacy Management Strategies and User Education}

	The question of how users actually manage their privacy in the context of wearables is more complicated than it first appears. While technical solutions and regulatory frameworks are often discussed, the reality is that much depends on the everyday actions, habits, and understanding of users themselves. Privacy management is a behavioral one, shaped by awareness, education, and the usability of available tools.

	A recurring finding is that most users are not fully aware of the privacy risks associated with wearable devices. Cilliers~\cite{Cilliers2019} provides clear evidence that many people do not understand what happens to their health data once it is collected, or even why it needs to be protected in the first place. This lack of awareness is a fundamental barrier to effective privacy management. When users do not know what is at stake, they are unlikely to take advantage of privacy settings or to question the default practices of device manufacturers. The result is a situation where risky behaviors like as sharing sensitive data without thinking or ignoring security warnings become normalized.

	Datta et al.~\cite{Datta2018} reinforce this point by showing that users often underestimate the sensitivity of the data they share, and are not always aware of the potential for re-identification or misuse. Their review highlights that privacy management is not just about having the right technical safeguards in place, but about making sure users understand the risks and know how to use the tools available to them. The authors argue for a holistic approach that combines technical solutions with clear, actionable privacy policies and ongoing user education. 

	One of the main challenges is the usability of privacy controls. Many wearable devices and their associated apps present privacy settings in ways that are difficult to find or understand, especially for users who are not technically inclined. This usability gap can lead to a false sense of security, where users believe their data is protected simply because they have clicked through a few menus, or to disengagement, where users give up on managing their privacy altogether. The literature suggests that privacy controls need to be designed with the user in minde, clear, intuitive, and accessible, with default settings that prioritize protection rather than convenience.

	Gupta et al.~\cite{Gupta2023} provide evidence that when users are given real choices like the ability to delete their data, control who can access it, or receive clear information about how it is used, they are more willing to share information and more proactive in managing their privacy. It shows that user empowerment is something that can be achieved through thoughtful design and communication. However, the challenge is that many platforms still treat privacy management as an afterthought, focusing on compliance rather than genuine user control. Because at the end of the day, it will cost companies money to implement these changes.

	Social awareness and peer influence also play a role in privacy management strategies. Li et al.~\cite{Li2019} show that users who are more influenced by the behaviors and expectations of their peers are more likely to engage in proactive privacy management, such as adjusting settings or limiting self-disclosure. This suggests that privacy education should not only focus on individual knowledge, but also leverage social dynamics to encourage best practices. In the context of wearables, where social sharing and community features are common, this dynamic can be particularly powerful.

	Despite these insights, significant barriers remain. Many users experience what the literature ~\cite{Choi2018} calls “privacy fatigue" which is a sense of being overwhelmed by the complexity and frequency of privacy decisions. This can lead to resignation, where users simply accept default settings or ignore privacy options altogether. The literature suggests that simplifying privacy choices, providing just-in-time education, and offering ongoing support are key strategies for overcoming privacy fatigue and fostering sustained engagement. This is especially relevant as wearables become more integrated into daily life and the volume of data collected continues to grow. But this requires a enormous collective consensus and effort from both users and companies.
\subsection{Innovative Technologies and Future Directions}
We have so far analyzed all the aspect of the current state of wearables, from ethical and regulatory frameworks to user behavior and decision-making. But what about the future? What innovative technologies and approaches are emerging that could potentially provide us with a path towards a more user-centric, privacy-respecting model of wearable health technologies? This section explores some of the most promising directions in the literature.
	\subsubsection{Decentralized Data Management: Blockchain, Personal Data Vaults, and User Empowerment}

	The rise of dataism and the explotion of connected devices slowly elevate the mindset towards the debate around who should control personal data. In the traditional model, data generated by wearables is collected, stored, and monetized by corporations, leaving users with little more than a vague sense of ownership and a long list of privacy policies that only a few reads. Alternatives that could restore some balance have started to move in this direction, proposing decentralized data management as a way to give users back control.

	One of the ideas is the Personal Data Vault (PDV), as introduced by Mun et al.~\cite{Mun2010}. The PDV is a privacy architecture that puts the user at the center. Instead of uploading raw data directly to service providers (like Google or Apple), individuals keep their data in a vault they control, filtering and sharing only what is necessary. The technical core of the PDV is the \textbf{granular Access Control List} (ACL), which lets users decide who can access their data, at what level of detail, and for what purpose. ACLs allow for real, detailled management. They also discuss a \textbf{trace-audit} feature, where every access to the data is logged, so users can see who has done what and when. There is also a \textbf{rule-recommender}, which suggests privacy policies based on past behavior, and is a practical addition, especially for those who are not privacy experts. However, wider deployment and public usage would need further backing and development.

	Building on this, Papadopoulou et al.~\cite{Papadopoulou2015} propose Personal Data Stores (PDS) and privacy policy negotiation. The PDS model is similar to PDVs but adds a layer of flexibility: users can negotiate with service providers about what data to share and under what conditions. As we have seen on the behavioral side, there is a lot a variation between individuals, and this negotiation allows for differentiated service levels, where users can choose to share more data in exchange for better services or keep things minimal for more privacy. The idea is to move away from the all-or-nothing approach that dominates most current platforms. Still, the challenge is to make these negotiations understandable and manageable for ordinary users, not just for tech-savvy individuals.

	The Polis framework, described by Efraimidis et al.~\cite{Efraimidis2008}, takes a slightly different approach. Here, personal data remains on the user’s side, and agents (software, not AI) negotiate access for each transaction. The benefit is that only the minimum necessary data is shared, supporting the principle of data minimization. This is interesting for wearables, where the temptation is always to collect as much as possible “just in case.” The modularity and scalability of Polis make it adaptable, but again, the question is whether users will actually engage with these systems or if the complexity will drive them away.

	Blockchain technology has been presented as a solution for many of these decentralized solutions. Zyskind, Nathan, and Pentland~\cite{Zyskind2015} lay out a system where personal data is encrypted and stored on a distributed ledger, with access permissions managed by smart contracts. Every transaction, like storing, sharing, or querying data, is recorded immutably, providing transparency and auditability. In theory, this removes the need for trust in a central authority, as the blockchain itself enforces the rules. However, the practicalities are less clear. Users must manage their own keys, which can be really complicated for non-technical individuals, and the scalability of blockchain systems is still a concern, especially when when we think about the high-frequency data streams generated by wearables.

	Recent work has tried to address some of these limitations by integrating blockchain with other technologies. Heister and Yuthas~\cite{Heister2021} show how combining blockchain with AI can enhance privacy and security. Decentralized identities managed on the blockchain allow users to authenticate themselves across platforms, while AI algorithms can detect threats and manage access dynamically. This is a step towards more adaptive and user-friendly systems, but it also introduces new risks, such as algorithmic bias or the opacity of AI decision-making.

	Boonsong et al.~\cite{Boonsong2024} review the application of blockchain-based intelligent wearable devices in the context of 6G systems. Their findings are optimistic: blockchain can secure patient records, ensure confidentiality, and support real-time health monitoring, reducing the need for hospital visits. The promise is that privacy and security can be embedded at the architectural level. But the literature is careful not to oversell the benefits. Scalability, usability, and interoperability with existing healthcare systems remain significant hurdles. There is also the issue of regulatory compliance—decentralized systems must still operate within the boundaries of laws like GDPR or HIPAA, which were not designed with blockchain in mind.

	Thus decentralized data management frameworks offer a credible path towards digital sovereignty. The technical mechanisms (granular access control, trace-audit, privacy policy negotiation, smart contracts) are all steps in this direction. They build concrete tools that can actually shift the balance of power. However, the effectiveness of these solutions is not only tech based. Usability, user education, and the willingness of service providers to participate are all critical. A technology might be promising, but fail to gain traction. There is a risk that, without proper support, these systems will remain niche solutions for privacy enthusiasts, while the majority of users continue to accept the status quo.

	Decentralized data management solutions, provides a direct response to the problems we have identified earlier. They could ensure that the shift towards Dataism can be done in a way that respects user autonomy and privacy. They challenge the dominance of centralized platforms and offer new ways for individuals to control, audit, and negotiate the use of their personal data. But, as we have seen, the adoption of these technologies is not guaranteed. A lot of actors need to come together to make this a reality. So even if the technical foundations are being laid, the real test will be whether users find these systems usable, trustworthy, and beneficial enough to change their behavior.
	\subsubsection{Privacy-Preserving Analytics: Federated Learning and AI Integration}

	Traditional models of centralized data collection and analysis, as we are seen, are not optimal for wearables. They raise significant privacy concerns, as sensitive health data is often stored in the cloud, making it vulnerable to breaches or misuse. The challenge is how to analyze this data without exposing it. Some methods have been developped 

	Zero-knowledge proofs are very interesting to analze. Introduced in the 1980s, they allow one party to prove to another that they know a value without revealing the value itself. In other words, ZKPs allow a user (the "prover") to prove to a service (the "verifier") that a statement about their data is true, \textbf{without revealing the data} itself. This could mean passing an airport security check without revealing personal information, or proving eligibility for a service without disclosing sensitive data. Later, Ben-Sasson et al.~\cite{BenSasson2019} made a significant contributions to this field, developing zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge). zk-SNARKs are a type of zero-knowledge proof that is particularly efficient, allowing for very short proofs that can be verified quickly. Building on it several projects have explored the combination of ZKPs with blockchain to enhance privacy in data sharing. For example, the Zcash cryptocurrency uses zk-SNARKs to allow private transactions, where the sender and receiver can be verified without revealing their identities or the transaction amount. This is a powerful example of how ZKPs can be applied to real-world problems, providing a way to maintain privacy while still allowing for verification and accountability. When combined with blockchain, ZKPs can provide users with complete ownership and control over their data, including the ability to revoke third-party access ~\cite{Zhou2023}. We also now have numerous cryptocurrency projects that use ZKPs to enhance privacy, such as Zcash ~\cite{BenSasson2014}. But still , the practical implementation of ZKPs in wearables is still in its infancy. The computational overhead and complexity of generating and verifying proofs can be significant, especially on resource-constrained devices like wearables. However, as hardware improves and algorithms become more efficient, ZKPs could become a viable option for privacy-preserving analytics in the future.
	Other promising ideas include the use of homomorphic encryption, which allows computations to be performed on encrypted data without decrypting it, and secure multi-party computation, where multiple parties can jointly compute a function over their inputs while keeping those inputs private. 

	Next we have Federated learning which is commonly assiociated with the training of machine learning (ML) and other predictive models. Now with the rise of AI, even in wearables, it might be interesting to look at it. The way it works is that instead of sending raw data to a central server, each device or institution keeps the data locally and only shares model updates, like gradients or parameters, with a central coordinator. Xu et al.~\cite{Xu2021} provide a comprehensive review of how this works in practice. The main advantage is that sensitive health data never leaves the user’s device or the hospital’s infrastructure, which reduces the risk of leaks or unauthorized access. The federated approach also helps address the problem of data heterogeneity, since each participant can contribute to the global model without having to standardize or pool their data in one place. However, the literature points to significant practical challenges. The communication overhead of training a model across thousands of devices creates latency and cost. Xu et al.~\cite{Xu2021} mention strategies like model compression to ease this burden, but these solutions are still maturing. Furthermore, model updates can leak information if not properly protected. While techniques like differential privacy are being integrated, they add complexity and can degrade model accuracy, leaving the classic trade-off between privacy and utility very much unresolved.

	The integration of AI with privacy-preserving architectures is also interesting. Xie et al.~\cite{Xie2021} show how combining AI, blockchain, and wearables can improve chronic disease management. In their system, wearable devices collect physiological data, which is then analyzed by AI algorithms to provide personalized health advice. Blockchain is used to secure the storage and sharing of this data, making sure that only authorized parties can access it and that all transactions are recorded transparently. The results were interesting, were continuous health monitoring, better disease management, and fewer hospital visits was observed. But again, the literature is careful not to oversell the benefits. The integration of these technologies introduces new risks, such as the opacity of AI decision-making and the potential for algorithmic bias. Users may not always understand how their data is being used or how decisions are made, which can undermine trust.

	Heister and Yuthas~\cite{Heister2021} take this a step further by exploring how AI can be used not just for analytics but also for security. Their framework combines decentralized identities managed on the blockchain with AI-driven threat detection and adaptive access management. The idea is that users can authenticate themselves across platforms and have their data protected in real time against emerging risks. This is a step towards more dynamic and user-friendly systems, but it also raises questions about scalability and the potential for new vulnerabilities. The more complex the system, the harder it is to guarantee that all parts are secure and working as intended.

	Thus, Zk-SNARKs, federated learning, and AI integration each bring their own strengths, but also their own limitations. The challenge is to integrate them in a way that actually delivers on the promise of user empowerment without creating new risks or barriers. Usability is a major concern, as we have seen, if the systems are too complex, users may not adopt them, or may use them incorrectly. There is also the risk that the burden of managing privacy will simply be shifted onto users, who may not have the technical skills or resources to keep up.
	\subsubsection{Incentivized Data Sharing and New Economic Models}

	Following on dencentralized storage solutions and privacy-preserving analytics, we here explore the motivational and economic layers.  We have seen that the individuals holds little power in the current centralized model, they are mainly inactive and do not benefit from the return on their data. The benefits are only that they have access to the service, but the economic value of the data is principally in the hands of a few actors. Thus, new economic models are emerging that aim to change this dynamic.

	The case of Steemit, as analyzed by Park ~\cite{Park2023}, a blockchain-powered social media platform where users are rewarded with cryptocurrency tokens for posting, curating, or sharing content, provides a good example. 
	Users receive tangible rewards, in this case Cryptocurrency Tokens, for their participation on the platform. Park’s findings shows that the presence of real incentives increases users’ willingness to disclose personal information, even when privacy concerns are present. However, the nature and size of the reward matter a lot. Small or unclear incentives do not really change behavior, but larger or more transparent rewards can tip the balance, making users more engaged and more willing to share. This suggests that money is not the only reward that matter, recognition, status, and the feeling of being part of a community that values your contribution seems to be important too.

	The implications of this model go beyond social media. Boonsong et al.~\cite{Boonsong2024} discuss how similar incentive structures can be applied to wearable health data. In their vision, users could receive token-based rewards for sharing their health data with researchers or public health initiatives. Smart contracts would automate the distribution of rewards and allow users to set detailed preferences about what data to share and with whom. This approach is supposed to empower users and create new opportunities for collective intelligence, as more people are motivated to participate. But the literature is careful not to ignore the risks. Incentives must be designed in a way that avoids exploitation or coercion. If the rewards are too high, users might underestimate privacy risks or share more than they should. If they are too low, the system will not work. There is also the risk that incentives could distort the original purpose of data sharing, turning it into a purely transactional activity rather than something based on trust or shared goals.

	The literature also highlights the importance of governance frameworks in these new models. Abraham, Schneider, and vom Brocke~\cite{Abraham2019} emphasize that incentives alone are not enough. There must be clear rules, processes, and responsibilities to ensure that data sharing remains ethical and user-centric. Without robust governance, there is a risk that incentives could lead to unintended consequences, such as data commodification or discrimination. The design of incentive structures must prioritize user autonomy, transparency, and fairness. Otherwise, the promise of empowerment could quickly turn into a new form of exploitation.

	Another point that emerges is the diversity of user motivations. Not everyone is motivated by financial rewards. For some, social recognition, access to better services, or the desire to contribute to research are more important. Park~\cite{Park2023} and Boonsong et al.~\cite{Boonsong2024} both note that effective incentive models should be flexible, allowing users to choose how and why they participate. This alignment of incentives with user values is important for building trust and ensuring that new economic models of data sharing support, rather than undermine, privacy and autonomy.

	Technical safeguards are also critical. By combining token-based rewards with privacy-preserving technologies, like anonymization, encryption, and decentralized storage, platforms can reduce the risk of misuse or re-identification. Smart contracts and blockchain transparency adds another layer of accountability, allowing users to verify how their data is used and by whom. But these technical solutions must be integrated, as already discussed, with user-friendly interfaces and clear communication, so that users understand the risks and benefits of participation.

	Incentives are thus an important factor that could make people adopt those technologies and influence how users percieve and manage their privacy while being secure. We discussed about the difficulty of getting users to engage with privacy controls and the complexity of current systems. Incentivized data sharing could provide a way to overcome these barriers, by making participation more attractive and rewarding. 
	But their success depends on careful design, robust governance, and the integration of privacy-preserving technologies. The future of data sharing is complex, especially as we move towards a dataistic society. Wearables could play a interesting role in this, but many challenges remain.
	\subsubsection{Technical, Usability, and Adoption Challenges for Future Solutions}

	The future of decentralized and privacy-preserving technologies for wearable health data sounds promising, based on everything that we discussed. All the innovative solutions that we have seen in the previous sections have difficult challenges. All are feasible technically, but many have strong barriers that related to usability, interoperability, and scalability, which can undermine even the most well-designed systems.

	One of the first issues is scalability. Blockchain-based systems, as we have seen, are often praised for their transparency and decentralized control, but they struggle when it comes to handling high volume and velocity data. Mun et al.~\cite{Mun2010} and Boonsong et al.~\cite{Boonsong2024} both point out that current blockchain infrastructures are not really designed for high-frequency, high-volume health data. Solutions like off-chain storage or sharding are being explored, but these introduce new problems around data integrity and synchronization. The same goes for federated learning frameworks, as Xu et al.~\cite{Xu2021} discuss: coordinating model updates across thousands of devices can create communication bottlenecks and increase energy consumption, which is not trivial when devices are battery-powered and users expect seamless performance.

	Interoperability is another challenge. As we have seen, the wearable ecosystem is fragmented, with a mix of proprietary platforms, data formats, and APIs. Papadopoulou et al.~\cite{Papadopoulou2015} emphasize that for decentralized solutions to gain real traction, they need to integrate smoothly with existing healthcare systems and legacy infrastructure. The lack of standardized data models makes it difficult to exchange information or aggregate data across devices, which limits the practical utility of personal data vaults or blockchain-based records. Achieving true interoperability requires collaboration between manufacturers, software vendors, and regulators, which is easier said than done.

	Regulatory and governance alignment is another complex topic. Decentralized solutions must navigate lors of national and international laws around data protection, consent, and cross-border transfers. Papadopoulou et al.~\cite{Papadopoulou2015} and Abraham et al.~\cite{Abraham2019} argue that effective governance frameworks are needed to clarify roles and responsibilities in decentralized ecosystems. Without clear rules for dispute resolution, data provenance, and ongoing compliance, even the best technical solutions may struggle to gain market entry or user trust. The literature suggests that incremental deployment, starting with hybrid models that combine centralized and decentralized elements, may be a pragmatic way forward, allowing stakeholders to build experience and confidence before scaling up.

	Also, the adoption of innovative data management technologies depends not only on users, but also on healthcare providers, insurers, researchers, and developers. Mun et al.~\cite{Mun2010} highlight the importance of pilot projects, co-design initiatives, and participatory governance models that involve stakeholders from the outset. Education and capacity-building are also needed, ensuring that all actors understand the benefits, risks, and operational requirements of new systems. Without this, there is a risk that promising technologies will remain stuck in the lab, never making the leap to everyday practice.

	We have also seen that usability and privacy fatigue remain significant barriers, particularly for vulnerable groups. We already talked about trust and how important it is for adoption and how it also involves demonstrating compliance with regulations
	Thus we are not going to elaborate here but it is a challenge that needs to be mentionned.

	Looking ahead, the literature is clear that overcoming these barriers will require a holistic, interdisciplinary approach. Technical innovation must be matched by advances in user-centered design, regulatory harmonization, and collaborative governance. Ongoing research should focus on real-world evaluations, longitudinal studies of user behavior, and the development of best practices for implementation and change management. Only by addressing these challenges directly can decentralized and privacy-preserving technologies move from theory to reality, fulfilling their potential to reshape the future of connected health in a way that genuinely empowers users.


	\begin{table*}[hbt]
		\caption{Centralized vs Decentralized Data Management in Wearables: Ethical and Privacy Implications}
		\centering
		\begin{tabular}{ll}
			\toprule
			\textbf{Dimension} & \textbf{Centralized} \hspace{1.5cm} \textbf{Decentralized} \\
			\midrule
			\textbf{Data Control} & \begin{tabular}[t]{@{}l@{}}Platform/corporate-owned; \\ limited user rights \end{tabular} \hspace{0.5cm} \begin{tabular}[t]{@{}l@{}}User-owned; \\ granular access/consent\end{tabular} \\
			\addlinespace
			\textbf{Privacy Risks} & \begin{tabular}[t]{@{}l@{}}High risk of breaches, \\ opaque data flows\end{tabular} \hspace{0.5cm} \begin{tabular}[t]{@{}l@{}}Enhanced privacy, \\ transparent logs\end{tabular} \\
			\addlinespace
			\textbf{Ethical Concerns} & \begin{tabular}[t]{@{}l@{}}Erosion of agency, \\ data commodification\end{tabular} \hspace{0.5cm} \begin{tabular}[t]{@{}l@{}}Digital sovereignty, \\ user empowerment\end{tabular} \\
			\addlinespace
			\textbf{Transparency} & \begin{tabular}[t]{@{}l@{}}Opaque terms, \\ limited user insight\end{tabular} \hspace{0.5cm} \begin{tabular}[t]{@{}l@{}}User-managed consent, \\ auditability\end{tabular} \\
			\addlinespace
			\textbf{Innovation} & \begin{tabular}[t]{@{}l@{}}Rapid, but \\ user autonomy loss\end{tabular} \hspace{0.5cm} \begin{tabular}[t]{@{}l@{}}Privacy-preserving \\ but adoption barriers\end{tabular} \\
			\bottomrule
		\end{tabular}
		\label{tab:centralized_vs_decentralized_brief}
	\end{table*}
	% \clearpage
	%------------------------------------------------

\section{Research Questions}

Here the research questions are presented, which will guide the empirical analysis and interpretation of results in the subsequent chapters. The primary question is framed to address the core concerns of user perceptions and adoption of decentralized data management technologies, while the secondary questions explore specific dimensions of this inquiry, including comparative user perceptions, drivers and barriers of adoption, demographic influences, and the role of incentives in shaping user attitudes.

\textbf{Primary Research Question:}

\begin{quote}
How do wearable device users perceive and evaluate decentralized data management technologies (such as blockchain and personal data vaults) as solutions to ethical, privacy, and autonomy challenges in the era of Dataism, and what factors most influence their willingness to adopt these innovations?
\end{quote}

\textbf{Secondary Research Questions:}

\begin{enumerate}
    \item \textbf{Comparative User Perceptions:} \\
    How do wearable users’ trust, perceived benefits, and willingness to adopt differ between centralized and decentralized data management models?
    
    \item \textbf{Drivers and Barriers of Adoption:} \\
    What factors (e.g., privacy concerns, incentives, usability, prior awareness) most influence wearable users’ willingness or reluctance to adopt decentralized personal data management solutions?
    
    \item \textbf{Demographic and Behavioral Segmentation:} \\
    How do demographic variables (such as age, education, country, and device usage) affect trust, perceived benefits, and adoption intentions for decentralized data management among wearable users?

	\item \textbf{Incentives and Motivations:} \\
	How does the privacy paradox shape wearable users’ attitudes and actual behaviors toward decentralized data management?
    
\end{enumerate}

\section{Methodology}

To answer the research questions, a mixed-methods approach was employed, mainly on quantitative analysis techniques but also with a bit of qualitative data collection. The methodology was designed to capture a comprehensive understanding of user perceptions, behaviors, and attitudes towards decentralized data management technologies in the context of wearable devices.
The reason for this approach is to leverage the strengths of both quantitative and qualitative methods, allowing for a more nuanced understanding of user attitudes and behaviors. The quantitative data provides measurable insights into user perceptions and adoption intentions, while the qualitative data offers deeper context and understanding of the underlying motivations and concerns.

\subsection{Data Sources}

The empirical analysis is based on a primary dataset collected through an online survey, designed to capture user attitudes, behaviors, and perceptions regarding wearable devices and decentralized data management technologies. The survey was distributed via social media channels, targeted online communities, and academic networks, ensuring a diverse sample of participants across different demographics and user types. The survey instrument included a mix of closed-ended Likert-scale items, multiple-choice questions, and open-ended responses to allow for both quantitative analysis and qualitative insights.
The primary dataset consist of approximately 240 survey responses with four different languages (English, French, Spanish, and German), different countries, age groups, and education levels. 

Secondary data sources were also used, notably the pew Research Center’s reports on 'How Americans View Data Privacy' about digital health and privacy, which provided a contextual backdrop for understanding broader trends in user attitudes toward technology and data management. The study was interesting to have a comparative perspective, as the data was collected via a large period, from 2021 to 2023, and with a large sample size (~5000). It is representative of the US population, smaller groups being overepresented (hispanic, black, and asian). A wider range of topic was addressed from security, privacy, AI and others. We can also note that there were incentives involved for participants and that the survey was of extremely high quality. This report helped to frame the survey questions and interpret the findings in relation to existing literature.
\subsection{Data Collection}
The data was collected over a period of one month, using Google Surveys as the platform for distribution and response collection. The survey was designed to be accessible on both desktop and mobile devices, and in multiple language, ensuring a wide reach and ease of participation. Participants were recruited through a combination of convenience sampling and targeted outreach, with an emphasis on reaching users of wearable devices across different countries and language groups. The survey was available in multiple languages (English, French, Spanish, and German) to accommodate the linguistic diversity of the target population.
Participants where recruited based on the available connections and networks, including social media platforms, online communities and on more local university networks, as well as professional groups related to health in switzerland.

\paragraph{Survey Questions}
The survey consisted of 26 questions, covering a range of topics related to user perceptions of centralized and decentralized data management, trust in technology, privacy concerns, and demographic information. The questions were designed to align with the research questions and capture both quantitative metrics (e.g., levels of trust, control preferences) and qualitative insights (e.g., open-ended responses about user experiences and expectations).

	The following table presents the full set of survey questions used to collect the primary dataset for this study. Each question is identified by its unique ID and a brief description.

	\begin{table*}[ht]
		\caption{Survey Questions Used in the Study}
		\centering
		\begin{tabular}{p{1.5cm} p{13cm}}
			\toprule
			\textbf{ID} & \textbf{Question Description} \\
			\midrule
			1 & How much do you trust current centralized systems (where companies store and manage your data) to protect your health information? \\
			2 & How much control do you feel you have over your data? \\
			3 & How much control do you feel users should have over their own health data? \\
			4 & How concerned are you about health data privacy? \\
			5 & Do you or did you use connected health devices (e.g., smartwatch, Garmin device, Apple device, fitness app tracker, smart scale, sport wrist band, Strava, etc.)? \\
			6 & (Non-users only) Why don’t you use connected devices? \\
			7 & What brand/app do you use? \\
			8 & (Users only) How do you balance health benefits against privacy concerns when using these devices? \\
			9 & (Users only) How often do you use your device(s)? \\
			10 & (Users only) Have you read your device’s privacy policy? \\
			11 & Prior to this survey, were you aware of these decentralized data management approaches? \\
			12 & If these systems allowed you to control who accesses your health data and how it's used, would you be interested in using them? \\
			13 & How much trust do you have in systems based on new technologies (like blockchain) to protect your data? \\
			14 & What would most concern you about adopting these new data management approaches? \\
			15 & How willing would you be to invest extra time or pay an additional fee to use a decentralized data management solution (such as a blockchain-based system or personal data vault) that gives you full control over your wearable health data? \\
			16 & Should users own their wearable data? \\
			17 & Are you willing to share data for health insights? \\
			18 & (If No) Why not? \\
			19 & Who do you believe should be primarily responsible for ensuring health data is used ethically? \\
			20 & Do you think those new methods to manage personal data will become mainstream in 5–10 years? \\
			21 & Do you have any other thoughts on data privacy or decentralized solutions? \\
			22 & Age group \\
			23 & Education level \\
			24 & Occupation \\
			25 & Country \\
			26 & Email for sending the summary \\
			\bottomrule
		\end{tabular}
		\label{tab:survey_questions}
	\end{table*}
	\clearpage

	% explain how it fits the research questions
	The data collection process was designed to align with the research questions, allowing for a comprehensive analysis of user perceptions and behaviors regarding decentralized data management in the context of wearable devices. The survey items were carefully crafted to capture both quantitative metrics (e.g., levels of privacy concern, trust scores) and qualitative insights (e.g., open-ended responses about user experiences and expectations).
\subsection{Data Analysis}
	\subsubsection{Introduction to Data Analysis}
	Data was analyzed using python, with a focus on descriptive statistics, subgroup comparisons, and correlation analyses and some quantitative methods. The analysis was structured around the four axes of the research design: ethical responsibilities, digital sovereignty, behavioral decision-making, and technological innovation. The analytical approach included:
	\begin{itemize}
		\item Construction of composite variables (e.g., privacy concern, control preference, trust in technology) based on survey items
		\item Segmentation analyses to identify patterns and differences across demographic groups and user types
		\item Correlation matrices to explore relationships between key variables (e.g., privacy concern and trust, control preference and adoption intentions)
		\item Subgroup comparisons to assess how attitudes and behaviors vary by country, language group, and device usage
		\item Exploratory factor analysis to identify underlying dimensions of user attitudes toward decentralized data management technologies
		\item Regression analyses to model the influence of demographic and behavioral factors on adoption intentions and perceived benefits of decentralized solutions
		\item Qualitative content analysis of open-ended responses to capture user sentiments, expectations, and concerns regarding decentralized data management
	\end{itemize}
	\subsubsection{Feature Engineering}
	To translate the raw survey responses into quantifiable variables suitable for statistical analysis, a set of engineered features was created. This feature engineering process was interesting for transforming qualitative attitudes and preferences into measurable metrics aligned with the core research questions. The primary derived features are:
	\begin{itemize}
		\item \textbf{Privacy Concern Score:} A composite index based on Q1--Q4, where higher values indicate greater privacy concern.
		\item \textbf{Control Preference Score:} The average of Q3, Q12, and Q15, reflecting the respondent's preference for user control.
		\item \textbf{Tech Trust Score:} Directly taken from Q13, measuring trust in new technologies.
		\item \textbf{Data Ownership Belief:} The reversed score of Q16, with higher values signifying stronger belief in user ownership.
		\item \textbf{User Type:} A binary variable ('User' or 'Non-user') derived from Q5.
		\item \textbf{Language Group:} Assigned based on language-specific keywords detected in the raw survey responses.
		\item \textbf{Response Completeness:} The percentage of non-missing responses for each participant.
	\end{itemize}
	The feature engineering workflow proceeded as follows: First, \texttt{user\_type} was determined by categorizing responses to Q5, and \texttt{lang\_group} was assigned by detecting language indicators in the original responses. 
	Next, attitudinal scores were constructed. The \texttt{privacy\_concern\_score} was computed by averaging Q1--Q4, with Q1 (trust in current systems) and Q2 (perceived control) inverted so that higher values consistently represent greater concern. This score was also categorized into levels ('Low', 'Moderate', 'High', 'Very High'). The \texttt{tech\_trust\_score} was mapped directly from Q13. The \texttt{control\_preference\_score} was calculated as the mean of Q3, Q12, and Q15, and the \texttt{data\_ownership\_belief} was obtained by inverting the 1--5 scale of Q16.
	Finally, \texttt{response\_completeness} was calculated by dividing the number of answered questions (excluding those skipped due to survey logic) by the total number of applicable questions, yielding a completeness percentage for each participant.	
	\subsubsection{Segmentation clusters}
	To segment respondents into distinct groups, a k-means clustering algorithm was employed. The determination of the optimal number of clusters proved to be complicated, as standard metrics like the inertia plot did not reveal a distinct 'elbow'. Thus, the silhouette score was prioritized as the primary criterion for model selection. While the overall clustering structure was weak, reflecting complex human attitudes, the 2-cluster solution provided the highest silhouette score (0.20). This score, though very modest, was substantially better than the alternatives. For instance, the 3-cluster solution scored a negligible 0.10, indicating significant overlap between segments.

	The 2-cluster is however not unusuable. Statistical analyses, as seen in the table \ref{tab:cluster_significance}, confirmed that the two clusters were statistically distinct across key variables. Indeed, ANOVA tests confirmed statistically significant differences (p < 0.05) between the two clusters across all core attitudinal variables, including privacy concern, control preference, and trust in technology, but except for data ownership belief. Similarly, Chi-Squared tests revealed significant demographic distinctions (p < 0.001) in user type, age, and occupation, confirming that the clusters represent empirically distinct populations. Thus, despite the weak signal, the 2-cluster solution provides the most statistically defensible and interpretable segmentation, offering an interesting narrative that will be explored.
	\begin{table*}[ht]
		\caption{ANOVA and Chi-Squared Tests: Significant Differences Between 2 Clusters}
		\centering
		\begin{tabular}{lcccc}
			\toprule
			\textbf{Feature} & \textbf{Test} & \textbf{Statistic} & \textbf{p-value} & \textbf{Significant?} \\
			\midrule
			privacy\_concern\_score    & ANOVA        & 7.60      & 0.0063  & Yes ($p < 0.05$) \\
			control\_preference\_score & ANOVA        & 10.81     & 0.0012  & Yes ($p < 0.05$) \\
			tech\_trust\_score         & ANOVA        & 17.22     & 0.0000  & Yes ($p < 0.05$) \\
			data\_ownership\_belief    & ANOVA        & 2.44      & 0.1199  & No ($p \geq 0.05$) \\
			willing\_to\_share         & ANOVA        & 29.26     & 0.0000  & Yes ($p < 0.05$) \\
			\midrule
			User Type                  & Chi-Squared  & 19.30     & 0.0000  & Yes ($p < 0.05$) \\
			Language Group             & Chi-Squared  & 113.22    & 0.0000  & Yes ($p < 0.05$) \\
			Age Group                  & Chi-Squared  & 179.06    & 0.0000  & Yes ($p < 0.05$) \\
			Education Level            & Chi-Squared  & 22.36     & 0.0002  & Yes ($p < 0.05$) \\
			Occupation                 & Chi-Squared  & 72.21     & 0.0000  & Yes ($p < 0.05$) \\
			\bottomrule
		\end{tabular}
		\label{tab:cluster_significance}
	\end{table*}
	\subsubsection{Correlation Analysis}
	The correlation analysis began by ensuring all key demographic variables and derived metrics were properly prepared, one-hot encoding categorical variables to make them match the numerical analysis. Two essential helper functions were then crafted: one to transform remaining categorical variables into numerical representations, and another to compute Pearson correlation coefficients between survey questions and our set of key variables. For each variable pairing, the analysis calculated both the correlation coefficient and its corresponding p-value. To maintain analytical rigor while focusing on meaningful relationships, only statistically significant correlations (p < 0.05) were retained, an easy but effective robustness check that aligned with our exploratory research objectives. The significant correlations were then presented in a structured table with clear significance level indicators and visualized through bar charts or heatmap, providing an accessible representation of the strength and direction of these relationships. These visualizations were saved as plot files for further analysis and inclusion in research outputs, that can be find in the supplementary materials. It's worth noting that this approach prioritized accessibility and interpretability over methodological complexity, we did not implement additional correlation types or advanced robustness checks, as our primary goal was to identify preliminary patterns in user perceptions of data privacy and control that could inform our understanding of attitudes toward decentralized technologies in in the age of Dataism.
	\subsubsection{Regression Analysis}
	To quantify the factors influencing key user attitudes, we developed a regression analysis framework comprising three distinct models, each tailored to a specific outcome variable. This statistical approach allowed us to move beyond simple correlations and identify the most influential predictors while controlling for demographic factors, which provided a more nuanced understanding of user perceptions.
	The three dependent variables required different modeling approaches based on their mathematical structure. Willingness to share data (Q17) was transformed into a binary variable (1 for 'Yes', 0 for 'No') and analyzed using logistic regression, which is the statistical equivalent of asking "will they or won't they?".
	Willingness to adopt decentralized solutions (Q12) was treated as an ordered categorical variable and examined through ordinal regression, respecting the stepwise nature of response options from complete disinterest to strong enthusiasm. And trust in blockchain technologies (Q13) was modeled with linear regression, treating the Likert scale as a continuous range of confidence.
	Across all models, we deployed a consistent set of predictors: privacy concern (composite score), control preference (composite score), trust in technology (where not the outcome), data ownership belief, age group, and education level. These variables weren't selected randomly as they represent the key features derived before and which were taken from the literature review and demonstrated evidence based distinctiveness in our correlation and factor analyses. The preparation process involved converting responses to appropriate numeric formats and encoding categorical variables (age and education) as integer codes to make them amenable to statistical analysis.
	To dive a bit more in depth, we identify a feature leakage in the construction of the \texttt{control\_preference\_score}. Indeed, the original composite included Q12, creating a circular relationship when predicting willingness to adopt decentralized solutions. To maintain analytical integrity, we reconstructed this predictor using only independent items (Q3 and Q15), ensuring our model results wouldn't be artificially inflated, respective good practices, espcially given the exploratory nature of our research into emerging technologies.
	Robustness was also  analysed. We tested for multicollinearity using Variance Inflation Factors (VIF) to confirm our predictors weren't just related to each other. Residual analysis was also analysed, and included an histogram visualization of error terms, which is good to verify model assumptions and assess goodness of fit. Additionally, likelihood ratio tests helped evaluate overall model significance, particularly important for the categorical outcomes.
	
	This multifaceted regression method enabled us to identify which factors most strongly influence users' willingness to adopt decentralized data management technologies.
	\subsubsection{Subgroup Comparisons}
    To understand how demographic and attitudinal characteristics shape perspectives on data privacy and decentralization, we developed a systematic subgroup analysis framework. The analysis began with the creation of specialized helper functions designed to examine response distributions across key demographic dimensions, like age, education, user type, language, and privacy concern level. These functions employed different methodologies depending on the question type: for single-answer questions, they generated percentage distributions within each subgroup, while for multiple-choice questions, they parsed individual selections and calculated proportional representations. This dual approach ensured that both exclusive choices and overlapping preferences were accurately captured. Data was preprocessed and the framework then systematically executed analyses across all question-subgroup combinations, creating a multidimensional matrix of results that revealed how perceptions varied across different population segments.
    The output is constructed to create structured reports containing question descriptions, distribution plots, key statistical metrics (means, medians, modes), and detailed cross-tabulation tables. For each subgroup analysis, the framework filtered out irrelevant categories to maintain analytical focus, generated comparative distribution visualizations, and produced percentage breakdown tables highlighting variations. 
	
	This methodical approach to subgroup analysis allowed us to identify significant differences in privacy concerns, trust levels, and adoption intentions across demographic segments.
	\subsubsection{Factor Analysis}
	To highlight the hidden shape of user attitudes, we conducted a multidimensional analysis of survey responses using complementary dimensionality reduction techniques. The factor analysis was performed on a selected subset of Likert-scale items, because they were easy to implement, and helped directly addressing our core research questions. These included trust in both centralized and decentralized systems (Q1, Q13), perceived and desired control over personal data (Q2, Q3), privacy concerns (Q4), willingness to adopt and invest in decentralized solutions (Q12, Q15), belief in user data ownership (Q16), and expectations for mainstream adoption (Q20). 
	
	We began the process by preparing the data, extracting the nine key Likert-scale questions from our survey dataset and converting them to numeric format. Any respondents with missing values for these questions were excluded. For the PCA, we applied StandardScaler to normalize each question's data distribution (mean=0, standard deviation=1), preventing questions with larger scales from dominating the analysis. 
	The exploratory factor analysis (EFA) was configured to identify three distinct factors using a 'varimax' rotation. To justify this approach, we generated a scree plot that help to visually plots eigenvalues against factors in descending order. According to the Kaiser criterion, which sets the minimum bar for significance where factors with eigenvalues exceeding 1.0 (marked by a red dashed line in our visualization) are considered substantive enough to retain, the three factors are justified. This quantitative threshold aligned with the visible "elbow" in our scree plot, confirming that three factors captured the meaningful variation without overcomplicating the model.	
	In parallel, we conducted a principal component analysis (PCA) on the standardized data, also extracting three principal components. The analysis quantified how much of the total variance in the data each component explained. To bring these abstract mathematical to an interpretable analysis, we created three distinct biplots (PC1 vs PC2, PC1 vs PC3, and PC2 vs PC3), each offering a different perspective on the multidimensional data. These visualizations positioned survey respondents as a constellation of points in the new component space, while the original Likert-scale questions appeared as directional arrows. The orientation and length of these arrows revealed how each question contributed to the principal components, with questions pointing in similar directions indicating positive correlation.

	This dual-method approach provided complementary insights into the structure of user attitudes toward data privacy and control. Reducing those questions into dimensions, we created a map of the attitudinal landscape that helped navigate the relationship between privacy concerns, control preferences, and openness to decentralized solutions in this complex digital environment.
	\subsubsection{Structural Equation Modeling (SEM)}
	Structural equation modeling which is a statistical approach that allowed us to test multiple causal pathways simultaneously, began with data preparation. We selected the relevant Likert-scale survey questions and converted them to numerical values, eliminating any records with missing responses to ensure analytical integrity. These responses were then standardized so that questions with different scales wouldn't dominate the analysis.
	
	Building on our factor analysis results, we created three composite scores that captured the essence of user attitudes:
	\begin{itemize}
		\item An openness score (averaged from three questions about technology adoption and trust)
		\item A privacy concern score (derived from two questions measuring data protection anxiety)
		\item A sense of control score (combining two questions about perceived agency over personal data)
	\end{itemize}
	With these building blocks in hand, we constructed a theoretical model reflecting our hypotheses about how these factors interact. Openness to decentralized technologies was positioned as the dependent variable, predicted by both privacy concerns and sense of control. Additionally, we theorized that control was itself influenced by privacy concerns, as maybe how worried people are about privacy might affect how empowered they feel over their data.
	After selecting only complete cases, we fitted the structural equation model to our data, but with a bootstrap procedure, resampling our data 2000 times and refitting the model on each sample. The final step involved generating a summary report, complete with parameter estimates and goodness-of-fit statistics, to interpret the results.

\section{Results}
\subsection{Introduction to Results}
% Briefly introduce the purpose and structure of the Results section.
% Summarize data sources, analytical approach, and limitations.
% Guide the reader on how the results are organized along the four axes.
The following section translates the conceptual and theoretical work into an analysis of empirical data. The analysis is organized along four axes that frame the research design: (1) ethical responsibilities, (2) digital sovereignty, (3) behavioral decision-making, and (4) technological innovation. The results aim at highlighing the links between user attitudes, perceived agency, and openness to decentralized solutions, while keeping an eye on the research questions regarding user perceptions, adoption barriers, and the shifting boundaries of trust and control in the age of Dataism. This section presents the results directly, letting the data speak for itself before moving to interpretation. Figures and tables are used to highlight key findings and reveal underlying patterns, with the primary goal of showing some visualizations that allow an easier comprehension of the data. All the materials (subgroup reports, correlation figures, regression tables, etc.) are available in the supplementary materials, and the code used for the analysis is provided in the code appendix.

The analysis begins with a description of the survey sample and the key metrics used, before proceeding axis by axis, moving from privacy concern and trust, through the persistent gap between desired and perceived control, to the behavioral calculus of data sharing and the landscape of decentralized innovation, and finally to the more advanced try at analysis techniques like factor, pca and SEM. Each subsection presents the empirical findings from the survey. A detailed interpretation of these results and their connection to the literature is reserved for the Discussion chapter.
\subsection{Sample Characteristics and Data Quality}
	\subsubsection{Demographic Overview}
	% Age, education, country, user type, language group
	As we have see, 240 participants completed the survey. As per the limits of the recruitment channels, the sample is skewed towards young adults, with over half (52.9\%) in the 25--34 age group. Younger respondents (18--24) make up 14.3\% of the sample, while older age brackets are less represented, with the minority being under 18 or over 65. The skew is also present when we look at the educational levels as the mahority of respondents (72.7\%) hold at least a bachelor’s degree, and nearly one in three report a master’s degree, which is consistent with the recruitment channels but it does introduces a strong selection bias that is addressed in the Limitations section.  
	
	The geographical distribution is dominated by Swiss residents (65.5\%), with a significant minority from Mexico (18.9\%) and a smaller group from France (4.3\%). While we do have a large variety of nationality represented, they often only account for a small number of respondents, which does not allow for meaningful subgroup analysis. For the languages present, we have an interestingly diverse sample, with French (127) being the most common language, followed by Spanish (49), English (40), and German (22). This mix of languages is a result of the study's international reach, but the uneven group sizes are considered in the analysis.
	
	In terms of user typology, 57.6\% of participants are classified as active users of connected health devices, while 42.4\% are non-users. Among users, daily engagement is the norm (65\%), and device preferences are concentrated among a few dominant brand: Apple (40.9\%) and Garmin (23.4\%), with other platforms and health apps occupying smaller niches. This concentration is not surprising given market trends, but it does mean that the perspectives of users of less common devices are underrepresented.
	
	The dataset is characterized by high response completeness, with most participants providing answers to the majority if not all of items. Structural missingness, primarily due to user- or non-user-specific questions, was systematically addressed through pre-specified logic in the data cleaning process, see code appendix, no imputation was performed for variables central to the main analyses. Composite and derived features, such as privacy concern, control preference, trust in technology, and data ownership belief, were constructed to enable segmentation and regression analyses, and their distributions are summarized in the following subsubsection.
	
	Overall, the sample is still interesting to analyse, even if its specific characteristics require caution when generalizing the findings. The study's limitations, including its exploratory nature and the overrepresentation of certain groups, are discussed in more detail in the Limitations section.
	\subsubsection{Key Features}
	% Distribution of privacy concern, control preference, trust, ownership belief
	Key features, as seen in Figure~\ref{fig:derived_features_summary} and detailled in the method section, were derived from the survey data in order to capture the main aspects of user attitudes and behaviors toward wearable data privacy. Interestingly, privacy concern score shows that over half of the respondents fall into the “High” or “Very High” concern categories, with a mean of 3.45 out of 5. Privacy concern by language group does not vary much, with German speakers having the highest mean of 3.83/5.
	Another related feature, control preference, shows a distribution that is quite neutral but slightly positive towards desiring more control with a mean of 3.6/5 and a median of 3.67/5.
	Data ownership belief is however heavily skewed towards high belief in user ownership. Indeed, more than sixty percent of respondents believe in data ownership.     

	These features, will later be useful in order to make further analysis.

	\begin{figure}[h!]
        \centering % Using \begin{figure*} makes the figure take up the entire width of the page
		\includegraphics[width=\linewidth]{figures/img/derived_features_summary.png}
		\caption{Summary of Key Analytical Features}
		\label{fig:derived_features_summary}
	\end{figure}
	\subsubsection{Segmentation}
This subsubsection presents the results of a cluster analysis designed to segment the sample into distinct user profiles. The aim here is to provide a structured, data-driven account of the diversity within the sample, highlighting how combinations of privacy concern, control preference, trust in technology, and data ownership belief can merge into interpretable attitudinal types.
To identify distinct attitudinal profiles within the sample, a k-means clustering algorithm was applied, resulting in a two-cluster solution that offers the most statistically defensible and interpretable segmentation. These two groups, which we label "Pragmatic Sharers" and "Skeptical" reveal divergent patterns in their engagement with technology, privacy attitudes, and demographic characteristics.

\paragraph{Cluster 1: Pragmatic Sharers (n=179)} This larger cluster represents a younger, technologically engaged group. A clear majority (65.9\%) are active users of connected health devices, and their demographic profile is dominated by the 25--34 age group (69.3\%) and students (35.2\%). Linguistically, this group is primarily composed of French (65.4\%) and English (21.8\%) speakers and has a higher average education level.

Attitudinally, this group is defined by a high willingness to share data (81.6\%) paired with the strongest preference for control over that data (mean score of 3.69/5). While their trust in technology is moderate (3.16), it is higher than the other cluster. Their privacy concern remains high (3.66), nearly matching the skeptical group, indicating that their willingness to share is not due to a lack of concern.

\paragraph{Cluster 2: Skepticals (n=59)} In an interesting contrast, this smaller cluster embodies a more cautious and distrustful stance. A significant majority (67.8\%) are non-users of connected health devices. Demographically, they represent an older group, with members concentrated in the 45--54 (30.5\%) and 55--64 (42.4\%) age brackets, and are predominantly Spanish (45.8\%) or German (35.6\%) speakers.

This group’s skepticism is reflected in their attitudinal scores: they exhibit the highest average privacy concern (3.94) and the lowest trust in technology (2.56). Consequently, their willingness to share data is the lowest observed, with only 47.5\% of members open to it. While their desire for control is moderate (3.35), their belief in user data ownership is strong (3.97), reinforcing their cautious orientation.

\begin{table*}[ht] 
	\caption{Summary of Key Cluster Results}
	\centering
	\begin{tabular}{lcc}
		\toprule
		& \textbf{Cluster 0: Skeptics} & \textbf{Cluster 1: Pragmatic Sharers} \\
		\midrule
		Size & 59 & 179 \\
		Privacy Concern Score & 3.94 (highest) & 3.66 \\
		Control Preference Score & 3.35 & 3.69 (highest) \\
		Tech Trust Score & 2.56 (lowest) & 3.16 \\
		Data Ownership Belief & 3.97 & 3.59 \\
		Willingness to Share & 47\% (lowest) & 82\% (highest) \\
		Main Age Groups & 45--64 (73\%) & 25--34 (69\%) \\
		User Type & 68\% Non-users & 66\% Users \\
		Dominant Language & Spanish/German & French/English \\
		Occupation & Employed/Retired/Self-employed & Student/Employed \\
		\midrule
		Stance & \parbox{6cm}{Highest privacy concern, lowest trust in technology, lowest willingness to share. Predominantly older, non-users.} & \parbox{6cm}{High willingness to share, strong control preference, moderate trust. Mainly younger, active users.} \\
		\bottomrule
	\end{tabular}
	\label{tab:cluster_summary_brief}
\end{table*}
	\subsubsection{Qualitative Insights}
		% Open-ended responses, word cloud, main themes
		Open-ended responses (Q21) offer a textured supplement to the quantitative findings, providing a window into the lived experiences and priorities of participants. Of the total sample, 38 respondents (15.8\%) contributed comments, with the majority centering on issues of control, trust, and transparency in the management of wearable health data. The language used is often direct and unambiguous, reflecting a persistent skepticism toward centralized data handlers and a pronounced desire for greater user agency.
		A word cloud visualization (\autoref{fig:q21_wordcloud}) highlights the most frequently used terms, with 'protect people' clearly standing out, and to a lesser degree "business", "layers", "big", "power", and "difficult". Several respondents explicitly referenced a lack of trust in corporations, using phrases such as “companies always find a way to exploit data” and “I don’t believe my data is safe, no matter what they promise.” Others emphasized the importance of user control, with comments like “I want to decide who sees my health data, not just accept terms” and “full control should be the default, not the exception.”
		
		\begin{figure}[h!]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q21_wordcloud.png}
			\caption{Word Cloud of Open-Ended Responses on Data Privacy and Decentralized Solutions (Q21)}
			\label{fig:q21_wordcloud}
		\end{figure}

		Recurring motifs include dissatisfaction with current privacy safeguards and a call for clearer, more accessible mechanisms to manage personal data. Several participants articulate frustration with the opacity of existing systems, noting difficulties in understanding who accesses their data and under what conditions. The demand for transparency can be seen in repeated references to the need for “clear rules” and “easy-to-understand privacy settings,” underscoring a gap between normative expectations and practical experience.
		A subset of responses also expresses interest in alternatives to mainstream platforms, with open-source and self-hosted solutions mentioned as preferable options for those seeking assurance over data handling. While some comments convey resignation, citing the challenge of keeping pace with evolving privacy policies, there is little evidence of apathy. Rather, the tone is one of cautious engagement, marked by a desire for meaningful control rather than passive acceptance.
		Table~\ref{tab:qualitative_themes} summarizes the main themes and representative quotes.

		\begin{table*}[ht]
			\caption{Main Themes from Open-Ended Responses (Q21)}
			\centering
			\begin{tabular}{p{3.5cm}p{11cm}}
				\toprule
				\textbf{Theme} & \textbf{Representative Quotes} \\
				\midrule
				Skepticism toward Centralization & “Companies always find a way to exploit data.”; “I don’t believe my data is safe, no matter what they promise.” \\
				Demand for User Control & “I want to decide who sees my health data, not just accept terms.”; “Full control should be the default, not the exception.” \\
				Transparency & “Clear rules are needed.”; “I want to know who accesses my data.”; “Privacy settings should be easy to understand.” \\
				Open-Source/Self-Hosted & “I would trust an open-source platform more than a big tech company.”; “Self-hosted data vaults are the only way to be sure.” \\
				Privacy Fatigue & “It’s impossible to keep up with all the changes.”; “I just accept the risks because I want the benefits.” \\
				\bottomrule
			\end{tabular}
			\label{tab:qualitative_themes}
		\end{table*}
\subsection{Axis 1: Ethical Responsibilities and Deontological Perspectives}
	\subsubsection{Privacy Concern}
		% Distribution and levels of privacy concern (Q4, composite score)
		% Subgroup differences (user type, language, age)
		Privacy concern is a significant and widespread sentiment within the sample. When asked directly about their concern for health data privacy (Q4), respondents showed a right-skewed distribution with a mean of 3.45 out of 5, indicating a moderate to high level of concern, as seen in \ref{fig:privacy_concern_q4}. Over half of the participants (52\%) reported high or very high concern, while a quarter (25\%) expressed low or no concern. A notable 23\% selected the neutral option, suggesting a significant degree of ambivalence.
		Correlation analysis confirms that privacy concern is strongly associated with several key attitudinal and demographic variables. The positive correlations are with the 'Skeptical' cluster ($r = 0.28$, $p < 0.001$) and control preference score ($r = 0.23$, $p < 0.001$). Conversely, concern is negatively correlated with the 'Pragmatic Sharers' cluster ($r = -0.28$, $p < 0.001$).
		Demographics play a clear role. Age is a significant factor, with concern increasing with age. The oldest groups (65+ and 55--64) show significant positive correlations ($r = 0.22$ and $r = 0.18$, respectively), while the youngest groups show significant negative correlations. Occupation mirrors this trend, with students exhibiting a strong negative correlation ($r = -0.26$, $p < 0.001$) and retired individuals a positive one ($r = 0.16$, $p < 0.05$). Language group is also a differentiator: Spanish speakers are positively correlated with higher concern ($r = 0.21$, $p < 0.001$), while French speakers are negatively correlated ($r = -0.19$, $p < 0.01$).
		Subgroup analysis further details these patterns. Non-users report slightly higher levels of ``very high'' concern (27.7\%) compared to users (21.2\%). An inverse relationship is observed with tech trust; as trust in technology increases, expressed privacy concern tends to decrease. However, no clear pattern emerges when comparing privacy concern with data ownership belief.
		\begin{figure}[h!]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q4_likert.png}
			\caption{Distribution of Privacy Concern (Q4: ``I am concerned about the privacy of my wearable data'')}
			\label{fig:privacy_concern_q4}
		\end{figure}
	\subsubsection{Trust}
		% Trust in companies (Q1), comparison with external benchmarks
		As the literature showed, trust is critical for the adoption of decentralized data management technologies. In this sample, trust was assessed using a single Likert-scale item (Q1: “How much do you trust current centralized systems (where companies store and manage your data) to protect your health information?”), showing a distribution centered at the neutral midpoint (mean = 2.67). Only a small fraction of respondents (~30\%) expressed “Complete trust,” while (40\%) indicated “Low” or “No trust.” The majority of responses clustered around neutrality, as visualized in Figure~\ref{fig:trust_q1}, with a slight right skew towards lower trust levels. 
		\begin{figure}[h!]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q1_likert.png}
			\caption{Distribution of Trust in Centralized Data Handlers (Q1: ``I trust companies to handle my wearable data responsibly'')}
			\label{fig:trust_q1}
		\end{figure}
		Correlation analysis reveals that trust in centralized systems is significantly associated with only three variables. The strongest relationship is a very strong negative correlation with privacy concern score ($r = -0.74$, $p < 0.001$). There is also a strong positive correlation with general tech trust score ($r = 0.46$, $p < 0.001$). Age shows a weak but significant negative correlation, with the 65+ age group being less trusting ($r = -0.15$, $p < 0.05$).
		Subgroup analyses reinforce these patterns. Trust levels decrease as privacy concern or control preference scores increase. Conversely, trust in centralized handlers increases with general trust in technology. Most demographic variables show minimal divergence, but age stratification confirms a modest trend where trust decreases in older age groups. The ``Pragmatic Sharers'' cluster exhibits a weak positive correlation with trust, while the ``Skeptics'' show a weak negative correlation.

		\paragraph{Regression}
		A linear regression model was estimated to identify the primary drivers of trust in new decentralized technologies (Q13). The model accounts for approximately 29.2\% of the variance in trust ($R^2 = 0.292$), indicating low-moderate explanatory power. Attitudinal factors are the most significant predictors.
		Two variables stand out with strong and statistically significant effects, but in opposite directions. The \textit{privacy concern score} is a strong negative predictor ($\beta = -0.61$, $p < 0.001$), meaning that higher privacy concern is associated with lower trust in new technologies. In contrast, the \textit{control preference score} is a strong positive predictor ($\beta = 0.59$, $p < 0.001$), showing that a greater desire for personal control over data corresponds to higher trust in decentralized solutions.
		Other variables in the model---belief in data ownership, age, and education level---did not show statistically significant effects on trust in new technologies.

		\paragraph{Trust in Data Handlers: Survey vs. Pew}
		A comparative look at trust in data handlers, using both the thesis survey and the 2023 Pew Research Center dataset, provides a useful external anchor for situating the empirical findings within a broader context. Both instruments probe the degree of trust individuals place in organizations responsible for managing their personal data, albeit with different reference points and response scales. In the thesis survey, respondents were asked to rate their trust in current centralized systems—where companies store and manage health information—on a five-point Likert scale. The Pew item, while focused on social media company leaders and their willingness to admit mistakes and take responsibility for data misuse, offers a parallel perspective on institutional trustworthiness in the digital domain.
		
		Figure~\ref{fig:trust_q1} visualizes the distribution of trust responses in the thesis sample. The pattern is centered at the neutral midpoint: 42.4\% of respondents selected “3,” indicating neither trust nor distrust. At the upper end, only 5.5\% expressed the highest level of trust (“5”), and 12.6\% chose “4,” together accounting for less than one-fifth of the sample. On the lower end, 22.7\% selected “2” and 16.8\% chose “1,” yielding a combined 39.5\% who report low or no trust in centralized data handlers. The overall distribution is thus marked by a concentration at neutrality, with a slight tilt toward skepticism.
		
		\begin{figure}[h!]\centering
			\includegraphics[width=0.7\linewidth]{figures/img/pew_comparison_plots/compare_1_vs_SMLEAD.png}
			\caption{Trust in Centralized Data Handlers: Survey vs. Pew Research Center (2023)}
			\label{fig:trust_pew_comparison}
		\end{figure}
		
		Turning to the Pew benchmark, the distribution is compressed into four categories and framed around trust in social media company leaders to admit mistakes and take responsibility for data misuse. Here, only 6\% of respondents report “A great deal” of trust, and 12.7\% indicate “Some” trust. The majority of responses are clustered at the lower end: 32.7\% select “Very little,” and 41.7\% choose “Not at all.” The Pew data thus reveal a pronounced deficit of trust, with nearly three-quarters of respondents expressing little or no confidence in the ethical conduct of data handlers.
		
		Direct comparison between the two samples requires some caution, given the differences in item wording, scale granularity, and the specific institutional focus. Nonetheless, several points of convergence and divergence are apparent. Both datasets reveal a scarcity of high-trust responses: only a small minority in each sample express strong confidence in the organizations managing their data. The Pew sample, however, displays a sharper polarization, with a larger share concentrated in the lowest trust category (“Not at all”) and a smaller proportion at the midpoint. In contrast, the thesis sample is characterized by a modal response at neutrality, with fewer respondents gravitating to the extremes.
		
		The central tendency in both datasets is clear: trust in data handlers—whether conceptualized as companies managing health information or as social media leaders—is limited. High trust is rare, and the prevailing sentiment is one of ambivalence or skepticism. The thesis sample’s distribution, with its pronounced midpoint, suggests a degree of uncertainty or withholding of judgment, whereas the Pew data reflect a more decisive lack of confidence.
		
		Subgroup analysis within the thesis sample, as detailed in earlier sections, indicates that trust in centralized data handlers is stable across user types, language groups, and age brackets. No substantial differences emerge by education or device usage frequency. The Pew dataset, while not segmented identically for this comparison, similarly points to a broad-based skepticism that cuts across demographic lines.
		
		Figure~\ref{fig:trust_pew_comparison} overlays the two distributions, highlighting both the shared scarcity of high-trust responses and the differing patterns of polarization and neutrality. The juxtaposition of these results underscores the empirical regularity of limited trust in institutional data handlers, regardless of the specific context or framing.
		
		In summary, the external benchmarking exercise situates the thesis findings within a wider empirical landscape. Both the thesis and Pew samples reveal that strong trust in organizations responsible for managing personal data is the exception rather than the rule. The thesis sample’s tendency toward neutrality contrasts with the Pew sample’s sharper tilt toward distrust, but the overarching message is consistent: confidence in data handlers remains fragile, and high trust is in short supply. These patterns provide a quantitative anchor for the subsequent analysis of privacy concern, control, and openness to alternative data management paradigms.
	\subsubsection{Responsiblity}
		% Who is responsible? (Q19)
		The question of who should bear primary responsibility for the ethical use of wearable health data reveals a divided but structured set of opinions. As shown in Figure~\ref{fig:responsibility_q19}, there is no single consensus. ``Shared responsibility'' was the most frequent choice (34.0\%), followed by ``Device manufacturers/companies'' (27.7\%), ``Individual users'' (20.2\%), and ``Government regulators'' (17.2\%).
		\begin{figure}[h!]\centering
			\includegraphics[width=0.7\linewidth]{figures/img/subgroup_ana/Q19_overall_dist.png}
			\caption{Distribution of Responsibility Attribution (Q19: ``Who should be primarily responsible for the ethical use of wearable health data?'')}
			\label{fig:responsibility_q19}
		\end{figure}
		Correlation analysis and subgroup distributions indicate that these attributions of responsibility are not random, but are linked to attitudinal and demographic profiles established throughout this study. The most pronounced effects are observed across user clusters. Placing responsibility on the ``Individual User'' is strongly and positively correlated with the 'Skeptical' cluster ($r = 0.32$, $p < 0.001$), with 50.9\% of this group selecting this option. Conversely, this choice is negatively correlated with the 'Pragmatic Sharers' cluster ($r = -0.32$, $p < 0.001$). The opposite pattern holds for attributing responsibility to ``Device Manufacturers,'' which is positively correlated with the 'Pragmatic Sharers' ($r = 0.14$, $p < 0.05$) and negatively with the 'Skeptics' ($r = -0.14$, $p < 0.05$).
		This division is mirrored in user typology. Device users are significantly more likely to hold companies responsible (35.8\% vs. 16.8\% for non-users) and less likely to choose individual responsibility (14.6\% vs. 27.7\%). This aligns with correlations showing 'user\_type\_User' is positively correlated with blaming manufacturers ($r = 0.21$, $p < 0.01$) and negatively with blaming individuals ($r = -0.16$, $p < 0.05$).
		Cultural and demographic factors are also significant. German speakers show a positive correlation with selecting ``Individual Users'' ($r = 0.27$, $p < 0.001$), with 54.5\% of them choosing this option, a stark contrast to other language groups. Age also shows a clear trend: the 65+ group is correlated with selecting individual responsibility ($r = 0.28$, $p < 0.001$), while the 18--24 group is negatively correlated with this choice ($r = -0.18$, $p < 0.01$) and positively correlated with holding manufacturers responsible ($r = 0.20$, $p < 0.01$).
		Attitudinal variables show weaker but consistent patterns. Higher privacy concern is weakly correlated with assigning responsibility to individuals ($r = 0.16$, $p < 0.05$). Lower trust in technology ($r = -0.17$, $p < 0.01$) and a lower preference for control ($r = -0.17$, $p < 0.05$) are also associated with choosing individual responsibility.
\subsection{Axis 2: Digital Sovereignty and Consumer Empowerment}
	\subsubsection{Perceived Control Over Data}
		% Perceived control (Q2), desired control (Q3), gap analysis
		A sense of disempowerment characterizes respondents' relationship with their wearable data. When asked to rate their perceived level of control (Q2), a clear majority (65\%) reported having ``No control'' or ``Low control,'' with a mean score of only 2.28 out of 5. Conversely, a mere 12.2\% felt they had ``High'' or ``Complete'' control, as illustrated in Figure~\ref{fig:perceived_control_q2}.
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q2_likert.png}
			\caption{Distribution of Perceived Control Over Wearable Health Data (Q2)}
			\label{fig:perceived_control_q2}
		\end{figure}
		Correlation analysis reveals that this perception of control is not arbitrary but is linked to underlying attitudes. The most significant factor is \textit{privacy concern score}, which has a strong negative correlation with perceived control ($r = -0.62$, $p < 0.001$). This inverse relationship is starkly visible in the subgroup distributions: among respondents with ``Very High'' privacy concern, 55.1\% report having ``No control,'' whereas no respondents in the ``Low'' concern group report this, and 50\% of them feel they have ``Complete control.''
		Conversely, a higher \textit{tech trust score} is positively correlated with a greater sense of control ($r = 0.19$, $p < 0.01$). Language group also emerges as a differentiator, with Spanish speakers feeling more in control ($r = 0.20$, $p < 0.01$) and French speakers feeling less so ($r = -0.17$, $p < 0.01$). Notably, key variables such as user type, age, and cluster profile do not show statistically significant correlations with perceived control, although subgroup distributions suggest ``Skeptical Non-Users'' feel slightly less control than ``Pragmatic Sharers.''
	\subsubsection{Perceived Control: Survey vs. Pew}
		A comparative analysis of perceived control over personal data, drawing on both the thesis survey and the 2023 Pew Research Center dataset, offers a external reference point for situating the findings within a broader context. While the two instruments differ in their precise wording and response scales, both probe the extent to which individuals feel they can influence the fate of their personal information in digital environments. In the thesis survey, respondents were asked, “How much control do you feel you have over your data?” using a five-point Likert scale, whereas the Pew item focuses on “How much control do you think you have over the data that companies collect about you?” with a four-point scale. Despite these differences, the underlying construct, perceived agency in the management of personal data, remains closely aligned.
		Figure~\ref{fig:pew_control_comparison} visualizes the distribution of responses in both samples. In the thesis dataset, as we have seen, the pattern is skewed toward the lower end of the control spectrum.			
		For Pew benchmark, the distribution is similarly weighted toward lower perceived control, though the scale is compressed. Only 4\% of Pew respondents report having “A great deal of control,” and 18\% indicate “Some control.” The majority of responses are concentrated in the “Very little control” (51\%) and “No control” (27\%) categories, yielding a combined 78\% who perceive themselves as largely powerless in the face of data collection by companies. The absence of a neutral midpoint in the Pew scale may contribute to the sharper polarization, but the overall pattern is consistent: high perceived control is rare, and the prevailing sentiment is one of limited agency.
		Direct comparison between the two samples requires some caution, given the differences in item phrasing and response options. Nonetheless, several points of convergence are evident. Both datasets reveal a pronounced skew toward low perceived control, with only a small minority expressing confidence in their ability to manage or influence the use of their personal data. The thesis sample, with its focus on wearable health data, displays a slightly higher proportion in the lowest category (“1”) compared to the Pew sample’s “No control,” but the overall shape of the distributions is remarkably similar. The central tendency in both cases is clear: most individuals do not feel in command of their digital footprints, whether in the context of health data or more general data collection by companies.
		In sum, the external benchmarking confirms that the observed deficit in perceived control is not unique to wearable health data users, but reflects a broader societal baseline. 		
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/pew_comparison_plots/compare_2_vs_CONTROLCO.png}
		\caption{Perceived Control Over Personal Data: Thesis Sample vs. Pew Research Center (2023)}
		\label{fig:pew_control_comparison}
	\end{figure}
	\subsubsection{Desired Control Over Data}
		% Desired control (Q3), distribution and subgroup differences
		In stark contrast to their perceived lack of control, respondents expressed an overwhelming and strong desire for greater agency over their data. When asked to rate their desired level of control (Q3), 85\% of respondents selected ``High'' or ``Complete'' control, resulting in a strongly left-skewed distribution with a mean score of 4.41 out of 5 (Figure~\ref{fig:desired_control_q3}). 
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q3_likert.png}
			\caption{Distribution of Desired Control Over Wearable Health Data (Q3)}
			\label{fig:desired_control_q3}
		\end{figure}
		This sentiment is consistent across most demographic and behavioral subgroups, including user type, age, and education, where no statistically significant patterns emerged.
		The desire for control is not an isolated preference but is strongly linked to underlying privacy attitudes. Correlation analysis reveals a positive relationship between desired control and the privacy concern score ($r = 0.52$, $p < 0.001$). This trend is illustrated in the subgroup analysis: as privacy concern intensifies, the demand for complete control escalates, with 90\% of respondents in the ``Very High'' concern category selecting ``Complete control,'' compared to only 25\% in the ``Low'' concern group. The control preference score, a composite variable of which this question is a part, shows a similarly strong positive correlation ($r = 0.46$, $p < 0.001$), confirming internal consistency. All other variables, including tech trust and data ownership belief, showed no statistically significant correlation with the desired level of control.
	\subsubsection{Belief in Data Ownership}
		% Data ownership belief (Q16), distribution and polarization	
		Respondents expressed a general belief that users should own the data generated by their wearable devices. When presented with this statement (Q16), a clear majority (60.2\%) either ``Strongly Agreed'' (51.3\%) or ``Agreed'' (9.2\%). In contrast, only 27.9\% disagreed, resulting in a distribution heavily skewed towards affirming user ownership (mean = 2.32, median = 1.00), as shown in Figure~\ref{fig:Q16_data_ownership}.
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q16_likert.png}
			\caption{Distribution of Responses: ``Users should own the data generated by their wearable devices'' (Q16)}
			\label{fig:Q16_data_ownership}
		\end{figure}
		A striking feature of this belief is its statistical independence from other core attitudes measured in this study. Correlation analysis reveals no significant relationship between the belief in data ownership and key variables such as privacy concern score, control preference score, or tech trust score. This independence is largely consistent across subgroups. While minor variations exist—for instance, the ``Skeptical Non-Users'' cluster shows slightly stronger agreement (61.0\%) than the ``Pragmatic Sharers'' (48.6\%), these differences do not translate into significant correlations. The only statistically significant, but weak, correlation is with the 55--64 age group ($r = -0.13$, $p < 0.05$), indicating a slightly stronger agreement with user ownership within this demographic.
\subsection{Axis 3: Behavioral Science and User Decision-Making}
	\subsubsection{Willingness to Share Data}
		% Willingness to share for health insights (Q17), main barriers (Q18)
		Analysis of willingness to share wearable health data for health insights (Q17) shows that attitudes and behaviors matter much more than demographics. Most respondents (73.1\%) say they are willing to share, but this openness is linked to a few key predictors, as seen in Figure~\ref{fig:Q17_share}.
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q17_single_choice.png}
			\caption{Willingness to Share Wearable Data for Health Insights (Q17)}
			\label{fig:Q17_share}
		\end{figure}
		Privacy concern is the main limiting factor, with a negative correlation to willingness to share ($r = -0.37$, $p < 0.001$). This is clear in subgroup results: among those with ``Moderate'' concern, 93.2\% are willing to share, but this drops to just 50.7\% for those with ``Very High'' concern.
		The  differences between the two main user clusters can alos be observed. ``Pragmatic Sharers'' are much more likely to share ($r = +0.33$, $p < 0.001$), with 81.6\% agreeing. ``Skeptics'' are the opposite ($r = -0.33$, $p < 0.001$), with only 47.5\% willing. This matches the user/non-user split: being a user is positively correlated ($r = +0.19$, $p < 0.01$), while being a non-user is negatively correlated ($r = -0.19$, $p < 0.01$).
		Tech trust also matters, though less strongly ($r = +0.20$, $p < 0.01$). Willingness to share rises from 47.6\% among those with the lowest trust to over 79\% for those with moderate or high trust.
		Demographics on the other hand play a smaller role. Younger respondents (18--24) are more likely to share ($r = +0.19$, $p < 0.01$), while older groups (45--54 and 65+) are less likely ($r = -0.17$ and $r = -0.18$, $p < 0.01$). Language group also matters: French speakers are more open ($r = +0.19$, $p < 0.01$), German speakers less so ($r = -0.26$, $p < 0.001$). Other factors—data ownership belief, control preference, education do not show significant effects.

		\paragraph{Barriers to Sharing}
		Among the 26.9\% of respondents unwilling to share their data, the cited barriers reveal a set of apprehensions. As shown in Figure~\ref{fig:Q18_barriers}, reluctance is primarily driven by fears of data ``Misuse'' (70.5\%), ``Privacy loss'' (68.9\%), a perceived ``No control'' over data (63.9\%), and ``Breach risk'' (52.5\%). The frequent selection of multiple barriers suggests these fears are deeply intertwined.
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q18_multiple_choice.png}
			\caption{Main Barriers to Sharing Wearable Data (Q18, multiple responses allowed)}
			\label{fig:Q18_barriers}
		\end{figure}
		Correlation analysis confirms that these barriers are not uniform across the sample but are strongly related to specific attitudinal and demographic profiles. Privacy concern is the most significant correlate, showing a strong positive association with citing all four barriers, especially ``No control'' ($r = 0.38$, $p < 0.001$) and ``Privacy loss'' ($r = 0.34$, $p < 0.001$). Conversely, higher tech trust is moderately negatively correlated with citing ``No control'' as a barrier ($r = -0.23$, $p < 0.001$).
		The two user clusters exhibit the most polarized patterns. The \textit{Skeptics} cluster is strongly and positively correlated with citing all major barriers (e.g., ``Privacy loss'': $r = 0.30$, $p < 0.001$), whereas the \textit{Pragmatic Sharers} cluster shows an equally strong negative correlation (e.g., ``Privacy loss'': $r = -0.30$, $p < 0.001$). This cleavage is mirrored in user typology, where non-users are significantly more likely to cite ``Misuse'' ($r = 0.19$, $p < 0.01$) and ``Privacy loss'' ($r = 0.18$, $p < 0.01$) as deterrents.
		Demographic factors also emerge as significant differentiators. Age is a consistent factor, with older groups (especially 65+) more likely to cite barriers like ``Privacy loss'' ($r = 0.25$, $p < 0.001$), while the youngest group (18--24) is negatively correlated with citing all four. Language group is also a strong discriminant; German speakers show a robust positive correlation with citing all barriers (e.g., ``Misuse'': $r = 0.30$, $p < 0.001$), while French speakers display a significant negative correlation, particularly with ``Privacy loss'' ($r = -0.23$, $p < 0.001$).

		\paragraph{Regression}
		A logistic regression model was estimated to identify the primary predictors of willingness to share wearable health data for health-related insights (Q17). The model demonstrates weak explanatory power (Pseudo R² = 0.19) and reveals that attitudinal factors are the most significant drivers.
		Privacy concern emerges as the dominant negative predictor ($\beta = -1.60$, $p < 0.001$). The result is significant, and the negative coefficient indicates that as a respondent's privacy concern increases, their likelihood of being willing to share data decreases substantially.
		In contrast, other core attitudinal variables do not show a statistically significant effect in this model. Control preference ($\beta = 0.32$, $p = 0.25$), trust in technology ($\beta = 0.07$, $p = 0.73$), and data ownership belief ($\beta = 0.13$, $p = 0.22$) are not significant predictors of the willingness to share data once privacy concern is accounted for.
		Among demographic variables, education level is a modest but significant positive predictor ($\beta = 0.42$, $p = 0.03$), suggesting that individuals with higher educational attainment are more likely to be willing to share their data. Age shows a marginally significant negative trend ($\beta = -0.21$, $p = 0.09$), hinting that older respondents may be less willing to share.
	\subsubsection{Willingness to Adopt}
	% Q12
	Analysis of willingness to adopt decentralized data management solutions (Q12) indicates a generally positive inclination among respondents. The mean response is 3.71, with a median of 4.0 on a five-point scale. Most participants reported they would be "likely" (39.5\%) or "very likely" (26.9\%) to use such systems if they provided enhanced control over personal data, as seen in Figure~\ref{fig:Q12_willingness}.
	\begin{figure}[h!]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q12_likert.png}
		\caption{Willingness to Adopt Decentralized Data Management Solutions (Q12)}
		\label{fig:Q12_willingness}
	\end{figure}
	Correlation analysis highlights several strong predictors of this willingness. The most influential is the \textit{control preference score}, which demonstrates a strong positive correlation ($r = 0.77$, $p < 0.001$). Subgroup distributions confirm a monotonic relationship: willingness to adopt rises directly with preference for control, with all respondents at the highest control preference also expressing the highest willingness. Trust in new technologies (\textit{tech trust score}) is another positive predictor ($r = 0.41$, $p < 0.001$).
	User clusters show polarization. The "Pragmatic Sharers" cluster is significantly and positively correlated with willingness to adopt ($r = 0.28$, $p < 0.001$), while the "Skeptical Non-Users" cluster is equally strongly negatively correlated ($r = -0.28$, $p < 0.001$). This pattern matches the finding that current device users are more willing to adopt ($r = 0.19$, $p < 0.01$) than non-users ($r = -0.19$, $p < 0.01$).
	Demographic factors reveal nuanced effects. Willingness is positively correlated with the 25--34 age group ($r = 0.20$, $p < 0.01$) and negatively with the 55--64 age group ($r = -0.17$, $p < 0.01$). Education shows a divergent pattern: holding a Master's degree is positively correlated with willingness ($r = 0.19$, $p < 0.01$), while holding a Doctorate (PhD) is negatively correlated ($r = -0.17$, $p < 0.05$). Subgroup data confirms that 40\% of respondents with a PhD are "not at all" interested.
	The relationship with privacy concern is non-linear and does not show a significant linear correlation. Subgroup analysis, however, reveals polarization among those with the highest privacy concern: they are both the most likely to be "very likely" to adopt (36.2\%) and have a notable share who are "not at all" interested (15.9\%).

	\paragraph{Regression}
	An ordinal regression model was build to identify the key predictors of willingness to adopt decentralized data management solutions (Q12),	with feature leakage corrected. The results indicate that attitudinal factors are the primary drivers of adoption intention.
	The strongest predictor is the clean control preference score (\texttt{control\_preference}), which exerts a highly significant positive effect on willingness to adopt ($\beta = 0.84$, $p < 0.001$). This suggests that a greater preference for personal control over data is strongly associated with increased interest in decentralized systems. Trust in new technologies (\texttt{tech\_trust\_score}) is also a significant positive predictor ($\beta = 0.66$, $p < 0.001$), indicating that higher trust correlates with greater willingness to adopt.
	Other variables, including privacy concern score ($\beta = -0.11$, $p = 0.636$), data ownership belief ($\beta = -0.03$, $p = 0.664$), age, and education level, do not show statistically significant effects. These findings suggest that adoption decisions are shaped more by proactive control preferences and trust in new solutions than by general privacy anxiety or demographic profile.
	\subsubsection{Privacy Policy Engagement}
	% Reading/skimming privacy policies (Q10), external benchmark
	Engagement with privacy policies among wearable device users is characterized by widespread disengagement. The data reveal a clear pattern of avoidance: a majority of users (67.6\%) report not reading policies at all, while 25.7\% indicate they only skim them. A small minority (6.6\%) claims to read them fully, as shown in Figure~\ref{fig:Q10_policy_engagement}.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q10_single_choice.png}
		\caption{Privacy Policy Engagement: Reading vs. Skimming (Q10)}
		\label{fig:Q10_policy_engagement}
	\end{figure}
	This behavior, however, is not uniform as correlation analysis shows it is associated with specific attitudinal and demographic profiles. A user's level of privacy concern is a key differentiator, showing a significant negative correlation with not reading policies ($r = -0.19$, $p < 0.01$). This indicates that individuals with higher concern are more likely to engage with policy documents. The effect is pronounced in subgroup analysis: 75\% of users with ``Low'' concern ignore policies, a figure that drops to 33.3\% for those with ``Very High'' concern.
	The user clusters also exhibit a significant relationship. The `Pragmatic Sharers' cluster is positively correlated with not reading policies ($r = 0.24$, $p < 0.001$), a finding that aligns with their more utilitarian approach to data sharing.
	Age emerges as another factor. The youngest group (18--24) is more likely to ignore policies ($r = 0.14$, $p < 0.05$), whereas the oldest (65+) is less likely to do so ($r = -0.16$, $p < 0.05$) and, conversely, more likely to read them in full ($r = 0.19$, $p < 0.01$). Education presents a more counter-intuitive pattern: users with a high school education were more likely to report reading policies fully ($r = 0.15$, $p < 0.05$), while those holding a Master's degree were slightly less likely to do so ($r = -0.13$, $p < 0.05$).	

	\paragraph{Privacy Policy Engagement: Survey vs. Pew}			
	A comparative look, as seen in Figure~\ref{fig:Q10_pew_comparison}, at privacy policy engagement showed that both datasets, in their own ways, demonstrated the extent to which individuals actively engage with privacy policies.
	In the thesis survey, as we have seen, most reported not reading the policy at all. In contrast, the Pew Research Center data, the question is framed around the act of clicking “agree” immediately, bypassing the content of the policy. The results showed that: 31\% report that they “always or almost always” accept without reading, 26\% do so “often,” and 22\% “sometimes.” Only 12\% say they “rarely” skip reading, and a scant 6\% claim they “never” accept without reviewing the terms. The cumulative effect showed that over three-quarters of respondents routinely accept privacy policies without meaningful engagement.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/pew_comparison_plots/compare_10_vs_PPREAD.png}
		\caption{Privacy Policy Engagement: Survey vs. Pew Research Center (2023)}
		\label{fig:Q10_pew_comparison}
	\end{figure}
	Comparing these two datasets, substantive engagement with privacy policies is the exception rather than the rule. In both samples, the majority of individuals either do not read or only superficially engage with the terms that define how their data may be used, shared, or monetized. The thesis survey, with its focus on wearable health devices, reveals even lower rates of full reading than the broader Pew sample, though the difference in question framing and context should be noted.
	\subsubsection{Behavioral Trade-offs: Health vs. Privacy}
	% Balancing health benefits and privacy concerns (Q8)
	Analysis of the health-privacy trade-off (Q8) reveals that users often prioritize or equally value health benefits over privacy. As shown in Figure~\ref{fig:Q8_tradeoff}, a majority of users are either "Equally concerned about both" or find that health benefits "somewhat" or "clearly" outweigh privacy concerns. The view that "Privacy concerns clearly outweigh health benefits" is the least common response, suggesting a pragmatic calculus among device users.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q8_single_choice.png}
		\caption{Trade-off Between Health Benefits and Privacy Concerns (Q8, device users only)}
		\label{fig:Q8_tradeoff}
	\end{figure}
	This trade-off is associated with attitudinal and demographic profiles. Attitudinal factors are influential. For instance, a higher privacy concern score is negatively correlated with prioritizing health benefits ($r = -0.31$, $p < 0.001$), indicating that as concern rises, the willingness to trade privacy for health diminishes. Conversely, higher trust in technology is positively correlated with prioritizing health benefits ($r = 0.17$, $p < 0.01$). The two user clusters also show a clear divergence: "Pragmatic Sharers" are positively correlated with health-benefit-oriented responses, while "Skeptics" are negatively correlated. Interestingly, a stronger preference for control is positively correlated with being "Equally concerned about both" ($r = 0.20$, $p < 0.01$), suggesting that a desire for agency is linked to a more balanced, rather than dismissive, view of privacy.
	Demographic factors also play a role, but a more modest one. Younger users and students are more likely to prioritize health benefits, with the 18--24 age group ($r = 0.27$, $p < 0.001$) and student occupation ($r = 0.24$, $p < 0.001$) both showing positive correlations with the view that "Health benefits somewhat outweigh privacy concerns." Language groups also show some variation; for example, French speakers are more likely to prioritize health benefits, while Spanish speakers tend to be more equally concerned. These subgroup distributions confirm that while the trade-off is a common feature of the user experience, its exact balance is shaped by a combination of underlying attitudes and demographic characteristics.
	\subsubsection{Device Usage Patterns}
	% User/non-user split, frequency, brand/app usage

	\paragraph{User/Non-User Split}
	Analysis of device usage (Q5) indicates that a majority of respondents (57.6\%) use or have used connected health devices.
	Several factors show statistically significant correlations with device usage. The \textit{tech trust score} ($r = 0.16$, $p < 0.05$) and \textit{control preference score} ($r = 0.13$, $p < 0.05$) are positively correlated with usage, indicating that higher trust and a greater preference for control are associated with being a device user. Conversely, \textit{privacy concern score} is negatively correlated with usage ($r = -0.15$, $p < 0.05$), suggesting higher privacy concerns are associated with non-use.
	Demographic factors also reveal distinct patterns. Device usage is highest among the 18--24 age group (73.5\%) and shows a significant negative correlation with the 65+ age group ($r = -0.14$, $p < 0.05$). Education level is also a factor, with usage being positively correlated with having a Bachelor's degree ($r = 0.15$, $p < 0.05$) and negatively correlated with having Upper Secondary/High School or Vocational training ($r = -0.13$, $p < 0.05$ for both). Among language groups, German speakers show a significant negative correlation with usage ($r = -0.17$, $p < 0.05$).
	The cluster profiles demonstrate the strongest associations with device usage, apart from the definitional \textit{user type}. The ``Pragmatic Sharers'' profile is positively correlated with usage ($r = 0.29$, $p < 0.001$), while the ``Skeptics'' profile has an equally negative correlation ($r = -0.29$, $p < 0.001$). The subgroup distribution report confirms these trends, showing that 65.9\% of Pragmatic Sharers are users, compared to only 32.2\% of Skeptics.

	\paragraph{Frequency of Device Usage}
	Among device users, engagement is high, with a majority (65.4\%) reporting ``Daily'' usage. This high-frequency engagement is not uniform across the sample and correlates significantly with several attitudinal and demographic factors, as seen in Figure~\ref{fig:Q9_usage_frequency}.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q9_single_choice.png}
		\caption{Device Usage Frequency Among Users (Q9)}
		\label{fig:Q9_usage_frequency}
	\end{figure}
	Attitudinally, daily usage is positively correlated with a higher \textit{tech trust score} ($r = 0.18$, $p < 0.01$) and negatively correlated with a higher \textit{privacy concern score} ($r = -0.21$, $p < 0.01$). Subgroup distributions confirm this: 88.9\% of users with the highest tech trust score use their device daily, compared to only 37.5\% of those with the lowest trust. Conversely, daily usage drops as privacy concern increases.
	The user clusters show a strong and polarized relationship with usage frequency. The ``Pragmatic Sharers'' cluster is significantly and positively correlated with daily usage ($r = 0.22$, $p < 0.001$), while the ``Skeptics'' cluster shows an equally strong negative correlation ($r = -0.22$, $p < 0.001$).
	Demographic factors also play a significant role. The 18--24 age group is positively correlated with daily usage ($r = 0.21$, $p < 0.01$), with 84\% of users in this bracket reporting daily use. Language is also a discriminant: English speakers are more likely to be daily users ($r = 0.16$, $p < 0.05$), whereas German speakers are less likely ($r = -0.16$, $p < 0.05$). Infrequent (``Rarely'') usage is weakly but significantly correlated with being a Spanish speaker ($r = 0.13$, $p < 0.05$), holding a Bachelor's degree ($r = 0.15$, $p < 0.05$), and being self-employed ($r = 0.16$, $p < 0.05$).

	\paragraph{Brands}
	The wearable device market within the user sample is highly concentrated, with Apple (40.9\%) and Garmin (23.4\%) being the most frequently reported brands (see Figure~\ref{fig:Q7_brand_app}). This brand preference is not random but correlates significantly with specific attitudinal and demographic profiles.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q7_multiple_choice.png}
		\caption{Brand and App Usage Among Device Users (Q7, multiple responses allowed)}
		\label{fig:Q7_brand_app}
	\end{figure}
	\begin{itemize}
		\item \textbf{User clusters:} The ``Pragmatic Sharers'' cluster is significantly and positively correlated with using Apple ($r = 0.20$, $p < 0.01$) and Garmin ($r = 0.14$, $p < 0.05$). Conversely, the ``Skeptics'' cluster shows an equally strong negative correlation with these two dominant brands.
		\item \textbf{Attitudes:} Garmin users show a significant negative correlation with both privacy concern score ($r = -0.16$, $p < 0.05$) and data ownership belief ($r = -0.15$, $p < 0.05$). In contrast, users of Fitbit and Xiaomi exhibit a positive correlation with a stronger belief in data ownership ($r = 0.15$ and $r = 0.13$, respectively; $p < 0.05$).
		\item \textbf{Demographics:} Apple is more prevalent among the 25--34 age group ($r = 0.15$, $p < 0.05$), while its usage is negatively correlated with the 45--54 age group ($r = -0.15$, $p < 0.05$). Spanish-speaking users are significantly less likely to use Garmin ($r = -0.17$, $p < 0.01$) and more likely to use ``Other device'' ($r = 0.20$, $p < 0.01$). Occupation also plays a role; for instance, self-employed individuals are strongly correlated with using ``Other device'' ($r = 0.27$, $p < 0.001$) and negatively correlated with using Apple ($r = -0.14$, $p < 0.05$).
	\end{itemize}
\subsection{Axis 4: Innovative Technologies and Future Directions}
	\subsubsection{Awareness of Decentralized Technologies}
		Awareness of decentralized data management approaches, such as blockchain or personal data vaults, is limited across the sample. A plurality of respondents (48.3\%) report being “No, not familiar” with these technologies, while 42.4\% are “somewhat familiar,” and only 9.2\% are “very familiar.” This knowledge gap, however, is not uniform and shows significant variation across specific demographic subgroups.
		Familiarity is positively correlated with being a device user. Users are significantly more likely to report being “very familiar” with these concepts than non-users ($r = 0.16$, $p < 0.05$), with subgroup data showing 13.1\% of users in this category compared to just 4.0\% of non-users.
		Education level presents a pattern as well. Respondents with “Vocational/Technical training or certificate” show a significantly higher level of awareness, being positively correlated with being “very familiar” ($r = 0.17$, $p < 0.01$). This group has the highest proportion of “very familiar” individuals (19.1\%) and the lowest proportion of those “not familiar” (29.8\%). Conversely, those with a Doctorate (PhD) report the highest rate of being “not familiar” (80\%).
		Language and occupation also act as differentiators. German speakers are significantly more likely to be familiar with these technologies than other language groups ($r = 0.16$, $p < 0.05$), with only 22.7\% reporting they are “not familiar,” compared to 59.2\% of Spanish speakers. Self-employed individuals are also more likely to be “very familiar” with decentralized approaches ($r = 0.15$, $p < 0.05$). No significant correlations are observed with attitudinal variables like privacy concern, tech trust, or control preference.
	\subsubsection{Trust in New Technologies}
		% Trust in blockchain-based systems (Q13), by education
		Trust in new decentralized technologies, such as blockchain, to protect wearable health data is characterized by a pronounced neutrality across the sample. The mean trust score is 3.01 on a five-point scale, with a plurality of respondents (40.8\%) selecting the neutral midpoint. While a slightly larger share expresses some level of trust (32.3\%) over distrust (26.9\%), high trust is rare (4.6\%), as seen in Figure~\ref{fig:Q13_trust_newtech}.
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q13_likert.png}
			\caption{Distribution of Trust in New Technologies (Q13: ``I trust systems based on new technologies (e.g., blockchain) to protect my wearable health data'')}
			\label{fig:Q13_trust_newtech}
		\end{figure}
		This ambivalent stance is not uniform and is significantly correlated with key attitudinal profiles. The strongest predictor of trust is privacy concern, which exhibits a moderate negative correlation ($r = -0.34$, $p < 0.001$). Subgroup analysis confirms this starkly: 100\% of respondents with the lowest privacy concern express trust, whereas 80\% of those with the highest concern express distrust. Conversely, a higher preference for control is correlated with higher trust in new technologies ($r = 0.33$, $p < 0.001$).
		The user clusters also show a significant and polarized relationship. The ``Pragmatic Sharers'' cluster is positively correlated with trust ($r = 0.26$, $p < 0.001$), while the ``Skeptics'' cluster is negatively correlated ($r = -0.26$, $p < 0.001$). This aligns with the finding that device users are significantly more likely to trust these technologies than non-users ($r = 0.16$, $p < 0.05$).
		Demographic factors play a lesser, though significant, role. The 25--34 age group shows a positive correlation with trust ($r = 0.15$, $p < 0.05$), while the 55--64 age group shows a negative correlation ($r = -0.17$, $p < 0.05$). Notably, respondents with a Doctorate (PhD) report the highest levels of distrust, with 40\% selecting the lowest trust score.
	\subsubsection{Trust in New Technologies: Survey vs. Pew}

	\paragraph{Trust in New Technologies: Survey vs. Pew}
	The Pew benchmark, asked respondents about their trust in companies to make responsible decisions regarding artificial intelligence in their products. While this question is broader than our specific focus on blockchain or decentralized systems, it taps into the same underlying concerns about technological stewardship and institutional accountability. The Pew results paint a picture of pronounced skepticism. Only 2\% of respondents report "a great deal" of trust—a strikingly small fraction. Another 22\% indicate "some" trust, while the vast majority express reservations: 41\% select "very little" trust and 30\% choose "not at all." In total, an overwhelming 71\% of respondents harbor little to no trust in companies' handling of AI. Trust, it seems, is in short supply.
	Our thesis sample tells a somewhat different story. As we've seen, neutrality dominates the landscape.
	Figure~\ref{fig:trust_newtech_pew} brings this comparison into visual focus, overlaying our survey results with the 2023 Pew Research Center data. The graph reveals that across both populations, high confidence in new technologies is rare, and most respondents cluster at the neutral or skeptical end of the spectrum.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/pew_comparison_plots/compare_11_vs_AI_HEARD.png}
		\caption{Trust in New Technologies: Survey Sample vs. Pew Research Center (2023)}
		\label{fig:trust_newtech_pew}
	\end{figure}
	The visual comparison highlights two distinct response patterns: our sample's "wait and see" neutrality versus the Pew respondents' more pronounced skepticism. But few people in either group express strong confidence in emerging technological systems, suggesting a fundamental wariness. 			
	In sum, both datasets tell us that strong trust in emerging technological systems—whether decentralized architectures or AI-driven platforms—is the exception rather than the rule.
	\subsubsection{Willingness to Pay or Invest}
	% Willingness to pay/invest for decentralized solutions (Q15)
	Willingness to invest time or money in decentralized data solutions is limited across the sample, with a mean score of 2.69 on a five-point scale. A plurality of respondents (30.7\%) are neutral, but a larger combined share expresses unwillingness (44.5\% selecting scores 1 or 2) than willingness (24.8\% selecting scores 4 or 5), as seen in Figure~\ref{fig:WTP_distribution}. 
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/questions/Q15_likert.png}
		\caption{Distribution of Willingness to Pay or Invest in Decentralized Data Solutions (Q15)}
		\label{fig:WTP_distribution}
	\end{figure}
	This general reluctance, however, is not uniform and is shaped by attitudinal factors.
	The single most dominant predictor is control preference, which exhibits a strong positive correlation with willingness to pay ($r = 0.76$, $p < 0.001$). The subgroup analysis confirms this: 100\% of respondents with the lowest control preference are unwilling to pay, while 100\% of those with the highest preference are willing. This indicates a direct relationship where the desire for control translates into a willingness to bear costs for solutions that provide it.
	Trust in new technologies is also a significant, though less powerful, positive predictor ($r = 0.27$, $p < 0.001$). Respondents with higher trust are more willing to invest, while those with the lowest trust are unwilling (57.1\% selecting score 1).
	The relationship with privacy concern is non-linear and does not appear as a significant linear correlation. Subgroup data shows that individuals with both the lowest and highest levels of privacy concern are the most unwilling to pay. For instance, 75\% of those with ``Low'' concern and 50\% of those with the highest possible concern score selected the lowest willingness score. Willingness appears to peak among those with moderately high, but not extreme, levels of concern.
	Demographic factors play a minor role. Age shows a weak but significant pattern, with the 25--34 and 35--44 age groups being slightly more willing to invest, while older (45--54, 55--64) and younger (18--24) groups are less willing. Education level reveals a counterintuitive finding: respondents with a Doctorate (PhD) are the most unwilling to pay, with 80\% selecting scores of 1 or 2.		
	\subsubsection{Expectations for Mainstream Adoption}
		Expectations for the mainstream adoption of decentralized data management technologies within the next 5–10 years are generally optimistic. The mean response is 3.64 on a five-point scale, with a median of 4.0, indicating a tilt towards likelihood. A majority of respondents (57.1\%) believe adoption is "likely" or "very likely" (scores 4 or 5), while only 14.7\% view it as unlikely, as seen in Figure~\ref{fig:mainstream_adoption_expectation}. 
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q16_likert.png}
			\caption{Expectations for Mainstream Adoption of Decentralized Data Management (Q16)}
			\label{fig:mainstream_adoption_expectation}
		\end{figure}
		This optimism, however, is not uniformly distributed and reveals significant patterns when analyzed by user cluster and demographics. The most interesting finding is the inverse relationship between cluster profile and expectation. The \textit{Skeptics} cluster, despite its general distrust, is positively correlated with expecting mainstream adoption ($r = 0.16$, $p < 0.05$). Conversely, the \textit{Pragmatic Sharers} cluster is negatively correlated with this expectation ($r = -0.16$, $p < 0.05$). Subgroup data confirms this: 37.3\% of Skeptics believe adoption is "very likely," compared to only 20.7\% of Pragmatic Sharers.
		Age also plays a significant role. The 55--64 age group shows the strongest positive correlation with expecting adoption ($r = 0.19$, $p < 0.01$), with 50\% of this group selecting the highest score. In contrast, the 25--34 age group ($r = -0.14$, $p < 0.05$) and students ($r = -0.19$, $p < 0.01$) are significantly less optimistic.
		Notably, core attitudinal variables such as privacy concern, control preference, and trust in new technologies do not show significant linear correlations with expectations for mainstream adoption. The subgroup distributions for these variables do not reveal a clear, monotonic trend, indicating that a respondent's general attitude toward privacy or technology does not directly predict their forecast for market-level adoption.
	\subsubsection{Barriers to Adoption}
		For barriers to adoption, the findings reveal that security and reliability concerns emerged as the most prominent barrier, with half of all respondents (50.6\%) expressing uncertainty about these fundamental aspects of decentralized systems, as seen in Figure~\ref{fig:Q14_barriers}. 
		\begin{figure}[ht]\centering
			\includegraphics[width=0.7\linewidth]{figures/questions/Q14_multiple_choice.png}
			\caption{Barriers to adoption of decentralized data management (Q14)}
			\label{fig:Q14_barriers}
		\end{figure}
		This skepticism was particularly pronounced among individuals with heightened privacy concerns ($r = 0.31$, $p < 0.001$) and those exhibiting lower trust in technology ($r = -0.27$, $p < 0.001$).
		The time burden associated with personal data management represented the second most frequently cited barrier (42.2\%), revealing a preference for convenience over control among a substantial portion of users. Interestingly, this concern was more prevalent among respondents with lower privacy concern scores ($r = -0.16$, $p < 0.05$).
		Complexity and usability challenges was also important (40.9\%), with Spanish-speaking respondents demonstrating significantly less concern about system difficulty ($r = -0.21$, $p < 0.01$). 
		Cost considerations, while less universal, still affected over one-third of respondents (34.2\%), with Spanish speakers showing heightened sensitivity to implementation expenses ($r = 0.14$, $p < 0.05$). This pattern, combined with their lower concern about complexity, suggests distinct priorities within different cultural contexts regarding the evaluation of new technologies.
		Integration challenges with existing devices represented a notable concern for specific user segments, particularly affecting Pragmatic Sharers ($r = 0.22$, $p < 0.001$) and current device users ($r = 0.16$, $p < 0.05$), while being less relevant for Skeptics ($r = -0.22$, $p < 0.001$) and non-users ($r = -0.16$, $p < 0.05$). 
		Age-related patterns showed that older adults (65+) demonstrated greater security concerns ($r = 0.15$, $p < 0.05$), reflecting perhaps a more cautious approach to new technologies developed through years of experience with technological change. Conversely, younger adults (25-34) prioritized integration compatibility ($r = 0.20$, $p < 0.01$) while showing less concern about security ($r = -0.14$, $p < 0.05$).
		These findings collectively reveal that the path toward decentralized data management faces resistance not merely from technological limitations, but from a complex web of user priorities, demographic characteristics, and practical concerns.
\subsection{Advanced Analysis}
	\subsubsection{Factor Analysis and Principal Component Analysis}
	The empirical landscape mapped in the preceding analyses is, by design, multidimensional. While regression and correlation models have clarified the main axes of association among key variables, they do not fully capture the latent structure underlying user attitudes and behavioral intentions. To address this, a factor analysis and principal component analysis (PCA) were conducted, aiming to distill the complex web of survey responses into a smaller set of interpretable dimensions. This approach allows for a more nuanced understanding of how the various attitudinal and behavioral constructs coalesce, overlap, or diverge within the sample.

	\paragraph{Factor Extraction and Loadings}
	The rotated factor solution yielded three factors with eigenvalues above 1, together accounting for approximately 54\% of the total variance (Factor 1: 24\%, Factor 2: 18\%, Factor 3: 12\%). The factor loading matrix, visualized in Figure~\ref{fig:factor_loadings_heatmap}, reveals a structure with minimal cross-loading, supporting the empirical distinctiveness of the extracted dimensions.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/factor_analysis/factor_loadings_heatmap.png}
		\caption{Factor Loading Heatmap (Varimax, 3 Factors)}
		\label{fig:factor_loadings_heatmap}
	\end{figure}
	\textbf{Factor 1}, which explains the largest portion of the variance, is defined by high positive loadings from questions concerning the willingness to use systems that provide greater control (Q12: 0.64), trust in new technologies like blockchain (Q13: 0.66), and the willingness to invest time or money for such solutions (Q15: 0.51). This dimension empirically captures an orientation of \textit{Openness to and Trust in Decentralized Solutions}.
	\textbf{Factor 2} is principally anchored by strong positive loadings on privacy concern (Q4: 0.63) and the desired level of user control (Q3: 0.42). The co-location of these items indicates that privacy anxiety and a normative demand for agency are empirically intertwined, forming a distinct dimension of \textit{Privacy Concern and Desire for User Control}.
	\textbf{Factor 3} is characterized by high positive loadings on perceived control over data within current systems (Q2: 0.61) and trust in those same centralized systems (Q1: 0.50). This factor represents an individual's degree of \textit{Trust and Perceived Control in the Current System}.
	Other items, notably the belief in user data ownership (Q16) and expectations for mainstream adoption of new technologies (Q20), exhibit low communalities and do not load strongly on any of the three factors, indicating their empirical independence from these primary attitudinal constructs.

	\paragraph{Principal Component Analysis and Dimensional Independence}
	The PCA results reinforce the factor analytic structure. The scree plot (Figure~\ref{fig:scree_plot}) shows a clear inflection after the third component, supporting the retention of three principal axes. The first three components together explain just over half of the total variance, with diminishing returns for additional components.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/factor_analysis/scree_plot.png}
		\caption{Scree Plot of Eigenvalues (PCA)}
		\label{fig:scree_plot}
	\end{figure}
	The biplots of principal components (Figures~\ref{fig:pca_biplot_pc1_vs_pc2}, \ref{fig:pca_biplot_pc2_vs_pc3}, and \ref{fig:pca_biplot_pc1_vs_pc3}) provide a visual summary of the relationships among items and components. And provide a visual confirmation of the independence of these components. The vectors representing the survey items radiate outwards from the origin in a star-like pattern rather than clustering, which indicates that the underlying dimensions are largely orthogonal. For instance, the biplot of PC1 versus PC2 shows that items loading on Factor 1 (Q12, Q13, Q15) project strongly along the PC1 axis, while items from Factor 2 (Q3, Q4) project primarily along the PC2 axis, confirming their distinctness. This geometric independence suggests that user attitudes are not reducible to a single dimension but are composed of multiple, non-overlapping constructs.
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/factor_analysis/pca_biplot_pc1_vs_pc2.png}
		\caption{PCA Biplot: PC1 vs. PC2}
		\label{fig:pca_biplot_pc1_vs_pc2}
	\end{figure}
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/factor_analysis/pca_biplot_pc2_vs_pc3.png}
		\caption{PCA Biplot: PC2 vs. PC3}
		\label{fig:pca_biplot_pc2_vs_pc3}
	\end{figure}
	\begin{figure}[ht]\centering
		\includegraphics[width=0.7\linewidth]{figures/img/factor_analysis/pca_biplot_pc1_vs_pc3.png}
		\caption{PCA Biplot: PC1 vs. PC3}
		\label{fig:pca_biplot_pc1_vs_pc3}
	\end{figure}
	\subsubsection{Structural Equation Modeling}
		Each preceding approach has contributed a distinct perspective on the relationships among privacy concern, control preference, trust in technology, data ownership belief, and behavioral intentions regarding wearable health data. To further interrogate the latent structure underlying these constructs and to test the plausibility of directional relationships among them, a structural equation modeling (SEM) approach was employed. This subsubsection details the rationale, model specification, estimation procedures, and empirical findings of the SEM analysis, while maintaining a focus on transparency and methodological rigor.

		\paragraph{Empirical Findings: Structural Model}
		The estimation of the structural model converged successfully; however, the analysis did not find statistically significant path coefficients for the hypothesized directional relationships. The direct effect of \texttt{factor\_privacy} on \texttt{factor\_openness} was estimated at $0.00$ (SE $= 0.05$, $z = 0.00$, $p = 1.0$). Similarly, the path from \texttt{factor\_privacy} to \texttt{factor\_control} was non-significant, with an estimated coefficient of $0.00$ (SE $= 0.05$, $z = 0.00$, $p = 1.0$). The path from \texttt{factor\_control} to \texttt{factor\_openness} was also estimated at $0.00$ (SE $= 0.06$, $z = 0.00$, $p = 1.0$).
		While the structural paths were non-significant, the variances of the latent constructs themselves were statistically significant, indicating internal consistency. The variance for \texttt{factor\_control} was estimated at $0.39$ ($p < 0.001$) and for \texttt{factor\_openness} at $0.33$ ($p < 0.001$). The bootstrapped confidence intervals for all structural path coefficients were wide and included zero, reinforcing the lack of statistical significance. These results empirically confirm the pattern of independence among the three core attitudinal dimensions that was previously suggested by the EFA and PCA, indicating that, within this dataset, the constructs operate as parallel rather than causally linked phenomena.

\section{Discussion}
\subsection{Synthesizing the Empirical Findings: A Portrait of the Contemporary Wearable User}
	% This section requires a high-level summary of the core empirical narrative. The selected sources establish the central tension: users are concerned, feel a lack of control, yet desire control and are open to innovation. They also introduce the two key user segments that challenge a monolithic view of the "user," which is a stated goal of this subsection.
	The empirical findings reveal a significant privacy concern among users of wearable devices, demonstrating a fundamental tension with the frictionless data flows idealized by Dataism. This concern, while not homogeneous across the sample, constitutes a core dimension of user identity formation. Two distinct attitudinal profiles emerge from the analysis: 'Skeptics,' who exhibit fundamental distrust and resistance toward the incumbent centralized paradigm, and 'Pragmatic Sharers,' who engage in a calculated benefit-risk evaluation prior to data disclosure. Notably, privacy anxieties stem predominantly from anticipated data misuse and technological distrust, rather than abstract notions of digital sovereignty. This insight provides crucial context for understanding the privacy paradox—the discrepancy between stated privacy values and actual disclosure behaviors. While broader societal data confirms the pervasiveness of these concerns, they manifest with diminished intensity in the health wearable domain due to perceived utilitarian benefits. Nevertheless, this anxiety engenders a substantial trust deficit, presenting a significant barrier for centralized data governance models and motivating exploration of alternative paradigms that enhance user agency.
	The empirical data further demonstrates that a majority of users perceive themselves as having negligible control over their personal data. This finding provides robust empirical validation for the literature's critique of eroded user agency in digital environments. A pronounced disconnect exists between the industry's empowerment narrative and the lived reality where users function primarily as passive data sources rather than sovereign actors. This agency deficit catalyzes heightened privacy anxiety, establishing a self-reinforcing cycle of disempowerment and distrust that transcends demographic and behavioral segments. The near-universal nature of this experience, affecting both 'Pragmatic Sharers' and 'Skeptics' alike, suggests a systemic failure of the current centralized model rather than a segment-specific phenomenon—a conclusion substantiated by external comparative data confirming this control deficit as a widespread societal baseline.
	Conversely, the data reveals an overwhelming and remarkably consistent demand for enhanced agency across demographic and behavioral strata. The pronounced preference for "High" or "Complete" control over personal data creates a stark disjunction between current perceived powerlessness and normative expectations of digital sovereignty. This desire for control exhibits a strong positive correlation with privacy anxiety, intensifying proportionally with concerns regarding potential data misuse. The statistical independence of this agency demand from technology trust and data ownership beliefs isolates it as a visceral response to the inadequacies of the current centralized paradigm, framing the willingness to adopt alternative solutions not merely as technological curiosity but as a direct pursuit of reclaimed autonomy.
	Despite prevailing privacy concerns and perceptions of disempowerment, the research identifies significant openness to innovative solutions that promise enhanced user agency. This receptivity is primarily driven by control preference and trust in emergent technological paradigms rather than mere technological enthusiasm. The empirical analysis suggests a nuanced relationship between privacy attitudes and adoption intentions: while elevated privacy concern inhibits data sharing within the current centralized framework, the promise of tangible control may redirect this same concern toward constructive interest in user-centric alternatives. This dynamic manifests in the divergent responses of 'Pragmatic Sharers,' who perceive decentralized systems as vehicles for enhanced empowerment, and 'Skeptics,' whose fundamental distrust remains a significant adoption barrier. These findings collectively suggest that the optimal developmental trajectory for data governance frameworks is one that prioritizes authentic, user-centric control mechanisms rather than abstract privacy assurances.
\subsection{Answering the Research Questions}
% This section is now explicitly structured to answer each research question sequentially, connecting the empirical results from the previous chapter to the theoretical frameworks and literature.
	\subsubsection{Addressing the Primary Research Question: Evaluating Decentralized Solutions}
	% This subsubsection directly answers the Primary RQ. It discusses how users evaluate decentralized technologies as a solution to the ethical, privacy, and autonomy challenges of Dataism. It will argue that while users are open to these solutions, their evaluation is primarily driven by a desire for control (a key factor), rather than a deep understanding of the technology itself. It will synthesize the regression and factor analysis results to pinpoint 'control preference' and 'tech trust' as the most influential factors for adoption.

	\paragraph{Privacy Concern}
	The empirical findings on willingness to adopt decentralized solutions provide a direct answer to the primary research question, revealing that users evaluate these innovations as potential instruments for reclaiming digital sovereignty. The analysis quantitatively confirms that the single most powerful driver of adoption is a user's preference for control, a factor that is statistically more influential than reactive privacy concerns. This suggests a reframing of the privacy calculus: while high privacy concern acts as a deterrent to sharing data within the current centralized model, the promise of tangible control can channel that same concern into a constructive interest in decentralized alternatives. This dynamic is validated by the polarization between the 'Pragmatic Sharers,' who see such systems as a logical step toward greater empowerment, and the 'Skeptics,' whose fundamental lack of trust remains a significant barrier. Ultimately, the willingness to adopt is less a reaction against privacy loss and more a proactive pursuit of agency, indicating that the most viable path forward for data governance is one that places user-centric control at its core.

	\paragraph{Perceived Control Over Data}
	However, this evaluation process is constrained by a significant knowledge gap, as the empirical data reveals a low baseline awareness of decentralized technologies across the sample. This unfamiliarity represents a primary obstacle, suggesting that the potential for these solutions to counter the rising Dataist paradigm is largely unrealized because users cannot demand or adopt alternatives they do not know exist. The analysis further show that awareness levels do not correlate with key attitudinal drivers like privacy concern or the desire for control. This implies that even the users most motivated to seek alternatives may be unaware that such solutions are emerging, reinforcing the argument that user education is an important prerequisite for adoption. While small pockets of higher awareness exist among specific subgroups, such as those with vocational training or current device users, the overall landscape is one of non-awareness, indicating that the "latent demand" for digital sovereignty is predicated on a level of technical literacy that is not yet widespread.

	\paragraph{Willingness to Adopt}
	Furthermore, the evaluation of these new technologies is shaped by a complex and often contradictory calculus of trust. The empirical data reveals that trust in decentralized solutions is not a given, but is characterized by a widespread neutrality, reflecting a "wait-and-see" attitude likely born from low awareness. This ambivalence is the result of two powerful, opposing forces. While high privacy concern dilutes confidence in any data-handling system, new or old, a proactive desire for control actively builds it. The regression analysis confirms that the promise of empowerment is as strong a positive driver for trust as privacy anxiety is a deterrent. This finding is interesting, as it suggests that overcoming user skepticism is less about assuaging abstract fears and more about providing concrete mechanisms for digital sovereignty, framing these technologies not as a defense against risk, but as an instrument for empowerment.
	\subsubsection{Addressing SRQ1: Centralized vs. Decentralized Models - A Comparative Analysis}
	% This subsubsection focuses on Secondary RQ1. It explicitly compares user attitudes towards the two models. It will contrast the low trust and low perceived control in centralized systems (Q1, Q2) with the high willingness to adopt (Q12) and higher trust in new technologies (Q13) when they promise control. This section directly addresses the comparative aspect of user trust, perceived benefits (control), and adoption willingness.

	\paragraph{Trust in Centralized Systems}
	The comparative analysis begins with the centralized model, which is defined by a significant and common trust deficit. The empirical data reveals a low level of trust in centralized data handlers, a finding that challenges the foundational principles of Dataism and validates the literature's consensus that trust is a fragile precondition for data sharing. This sentiment is not an isolated attitude but is linked with privacy anxiety. The negative correlation between the two suggests they are two sides of the same coin in the user's mind. This lack of trust appears to be a generalized sentiment, influenced more by a user's broad faith in the technological ecosystem than by a detailled evaluation of specific platforms, a dynamic indicative of "privacy fatigue." External comparisons with broader societal data confirm that this scarcity of high trust is a common phenomenon, establishing a low-trust baseline that represents a market failure for the centralized model and serves as a primary driver for exploring the decentralized alternatives.

	\paragraph{Perceived Control Over Data}
	With the common feeling of having little to no control over personal data, as noted by the results, a powerful observed validation for the literature's critique of Dataism can be made. This finding quantifies the "erosion of user agency" \cite{VanDijck2014}, revealing a stark disconnect between the industry's narrative of empowerment and a lived reality where users feel more like passive data sources than sovereign actors. This lack of agency is not a passive sentiment but a primary driver of privacy anxiety, creating a cycle of distrust and disempowerment that a general faith in technology is insufficient to overcome. Most importantly, this feeling of powerlessness is a near-universal experience that goes beyond user segments, affecting both engaged 'Pragmatic Sharers' and cautious 'Skeptics' alike. This reinforce that the issue is a systemic failure of the current centralized model.

	\paragraph{Desired Control Over Data}
	This sense of disempowerment is directly countered by a near-unanimous and demand for greater agency, which stands as a powerful pushback to any notion of user apathy. The preference for "High" or "Complete" control over personal data is not a niche sentiment but a remarkably consistent demand, creating a big gap between the reality of perceived powerlessness and the normative expectation of digital sovereignty. This desire for control is by nature linked to privacy anxiety, intensifying as concern over data misuse grows. The fact that this demand for agency operates independently of general tech trust or abstract beliefs in data ownership further isolates it as a core, instinctive response to the inadequacies of the current centralized model, framing the willingness to adopt new solutions as a direct pursuit of reclaiming lost control.

	\paragraph{Willingness to Adopt}
	Finally, this landscape of concern and disempowerment does not end up in indifference, but in a willingness to adopt new solutions, provided they restore user agency. This openness to innovation is primarily predicted by a user's preference for control and their trust in new technological ways of thinking. The regression analysis confirms by the numbers that the desire for control is a more influential driver of adoption than reactive privacy concerns. As explained before, this means that high privacy concern discourages sharing in centralized systems, but the promise of user control can motivate interest in decentralized alternatives, especially among those seeking empowerment rather than those who remain distrustful. 
	\subsubsection{Addressing SRQ2: Drivers and Barriers to Adopting Decentralized Solutions}
	% This subsubsection tackles Secondary RQ2. It will detail the primary drivers for adoption, identified as control preference and tech trust. It will then analyze the main barriers identified in Q14 (security concerns, complexity, time), linking them to low prior awareness (Q11) and interpreting them in the context of the literature on usability and trust. It will also explicitly acknowledge that incentives were not directly measured, a limitation to be addressed later.

	\paragraph{Drivers for Adoption}
	The regression analysis provides an interesting answer to the second secondary research question regarding the factors that influence the adoption of decentralized technologies. The model confirms by the numbers that the most powerful driver is the user's preference for control. This finding supports the thesis's building argument, that in the coming age of Dataism, the path to user acceptance for new data mindsets is not about softening privacy fears, but about actively empowering users with tangible agency. The second most influential factor is trust in new technologies, which, while less powerful than control preference, still underscores that trust is important for adoption. The statistical significance of control preference and trust in new technologies positions digital sovereignty and confidence in innovation as the core pillars for adoption. These results offer an interesting distinction between reactive privacy concern and a proactive desire for control, suggesting that while high privacy concern may drive reluctance to share data within existing centralized systems, it does not, alone, translate into a willingness to adopt new ones. Instead, adoption is built on a constructive desire for control and a belief in the efficacy of new technological solutions, a finding that moves the discourse beyond the privacy paradox and toward a more nuanced understanding of user motivation.

	Furthermore, the factors influencing the adoption of these new technologies is shaped by a complex and often contradictory calculus of trust. The data collected reveals that trust in decentralized solutions is not a given, but is characterized by a common neutrality, reflecting a "wait-and-see" attitude which might be born from low awareness. This mixed feelings is the result of two powerful, opposing forces. While high privacy concern dilutes confidence in any data-handling system, new or old, a proactive desire for control actively builds it. The regression analysis confirms that the promise of empowerment is as strong a positive driver for trust as privacy anxiety is a deterrent. This finding is interesting, as it suggests that overcoming user skepticism is less about calming down abstract fears and more about providing concrete mechanisms for digital sovereignty, framing these technologies not as a defense against risk, but as an instrument for empowerment.

	\paragraph{Barriers to Adoption}
	However, significant barriers to adoption persist, rooted in the same anxieties that define the current centralized paradigm. The primary concerns cited by users (fear of data misuse, loss of privacy, security breaches, and a fundamental lack of control) are not abstract dealbreakers to technology but are rational responses to the perceived failures of the existing data governance model. These fears are the tangible expression of a broader privacy concern, with the feeling of disempowerment being a particularly big speed bump. This is validated by external data showing that nearly half the public has abandoned services over such issues, confirming that these barriers translate into concrete, market-shaping behavior. This landscape of distrust, particularly acute among 'Skeptics' (one of our segments), establishes a high threshold for any new technology, indicating that decentralized solutions must not only offer novel features but also convincingly demonstrate their ability to overcome the security and agency deficits that haunt their centralized predecessors.

	Finally, as previously established in the portrait of the contemporary users section above, a foundational barrier is the widespread user unfamiliarity with decentralized technologies, which prevents even motivated individuals from considering these solutions.
	\subsubsection{Addressing SRQ3: The Role of Demographic and Behavioral Segmentation}
	% This subsubsection is dedicated to Secondary RQ3. It will analyze how demographic variables and the two identified user clusters ('Pragmatic Sharers' and 'Skeptics') influence attitudes. It will discuss the significant findings related to age, language group, and user type on variables like willingness to share, trust, and adoption intention, thereby providing a complete answer to the question of segmentation.
	Our analysis reveals significant demographic and behavioral patterns that shape user attitudes toward wearable technologies and data privacy. The collected evidence demonstrates a common privacy concern among participants, impacting the dataistic assumption that information should flow freely without restriction. This concern, however, manifests unevenly across our sample, taking shape into two distinct attitudinal profiles that we have designated 'Skeptics' and 'Pragmatic Sharers.' The former exhibit pronounced privacy anxiety and resistance to prevailing centralized data governance models, whereas the latter engage in calculated risk-benefit evaluations prior to disclosure decisions. Age emerges as a particularly influential variable in this segmentation, with older respondents demonstrating higher privacy vigilance, while younger participants and students display markedly reduced concern. This suggests a generational normalization of data sharing practices. When examining responsibility attribution patterns, we observe another divergence: 'Skeptics,' who tend to be older and inherently distrustful, advocate for individual data responsibility, reflecting their defensive posture toward institutional actors they perceive as untrustworthy. Conversely, 'Pragmatic Sharers,' typically younger and more technologically engaged, assign primary responsibility to device manufacturers, indicating an expectation of ethical stewardship from commercial providers. These fundamental attitudinal differences directly translate into sharing behaviors, with cluster membership proving more predictive than traditional demographic variables. 'Pragmatic Sharers' willingly exchange personal data for perceived health insights, while 'Skeptics' exhibit pronounced reluctance stemming from systematic distrust. Looking toward innovative solutions, we find that desire for enhanced control represents the primary motivator for potential adoption of decentralized technologies across segments, with even privacy-conscious users demonstrating openness to alternatives that promise authentic agency. The 'Pragmatic Sharers' perceive such systems as mechanisms for enhanced empowerment, while 'Skeptics' maintain their characteristic wariness. The fundamental decision to adopt wearable devices itself reveals a segmentation pattern, with lower privacy concerns and higher technological trust correlating with usage, and younger, more educated demographics taking the center stage among users, which further confirms the division between 'Pragmatic Sharers' and 'Skeptics.' Among device adopters, we observe mainly a daily usage patterns associated with reduced privacy concern and elevated trust, indicating that for regular users, perceived benefits consistently outweigh perceived risks. This engagement intensity varies demographically, with younger users more likely to maintain daily device relationships and certain language groups (particularly German speakers) demonstrating reduced usage frequency, reflecting how cultural privacy perspectives tangibly impact behavior. Finally, brand selection itself operates as an expression of underlying privacy values, with market leaders like Apple and Garmin disproportionately favored by 'Pragmatic Sharers,' signaling comfort within established ecosystems, while 'Skeptics' actively avoid these dominant platforms, manifesting their resistance to mainstream data governance paradigms.
	\subsubsection{Addressing SRQ4: Privacy Paradox}
	% How does the privacy paradox shape wearable users’ attitudes and actual behaviors toward decentralized data management?
	% This subsubsection will explicitly address the privacy paradox.

	\paragraph{Privacy Policy Engagement and the Privacy Paradox}
	The findings on privacy policy engagement reveal a pronounced manifestation of the privacy paradox. Empirical data show that an overwhelming majority of users (67.6\%) completely disengage from privacy policies, while only 6.6\% report reading them fully. This disengagement is shows a pattern as 'Pragmatic Sharers' are more likely to avoid policies, and younger cohorts are especially prone to ignoring these documents.
	External comparison with the 2023 Pew Research Center data confirms that this behavior is not isolated but a pervasive feature of digital interaction, empirically validating critiques of the notice-and-consent framework. Widespread non-engagement challenges theoretical models that assume rational, informed actors, and instead points to a structural failure: users, faced with opaque and non-negotiable terms in the Dataist paradigm, experience "privacy fatigue" and resort to perfunctory acceptance without meaningful review.

	\paragraph{Willingness to Share Data}
	This finding justifies the exploration of decentralized alternatives that plant privacy protection at the architectural level, rather than relying on a fundamentally broken consent model. The logistic regression results on willingness to share data reveal a manifestation of the privacy paradox, providing evidence that user behavior in centralized systems is primarily governed by reactive risk aversion rather than proactive agency-seeking. The overwhelming significance of \textit{privacy\_concern} as a negative predictor validates the privacy calculus literature, while the striking non-significance of \textit{control\_preference} shows a fundamental disconnect between users' abstract desires and actual sharing decisions. This distinction offers a powerful explanatory mechanism for the privacy paradox: users may express strong preferences for control in principle, yet find no meaningful outlet for this preference within current ``take it or leave it'' sharing paradigms, causing them to default to decisions based purely on their level of privacy concern. The regression thus demonstrates that within the centralized model, sharing behavior is defined by what users fear losing (privacy) rather than what they hope to gain through agency (control), a defensive rather than aspirational calculus. This empirical finding provides an interesting baseline for understanding user motivation and highlights why decentralized technologies, explicitly designed to shift this calculus from risk mitigation to active empowerment, may represent a necessary evolution rather than a simple alternative approach to data governance.
	
	\paragraph{Health Privacy Tradeoff}
	The empirical investigation into the health-privacy tradeoff directly shows a manifestation of the privacy paradox, revealing how users actively negotiate competing values. The data demonstrates a normalization of privacy concessions among active wearable users, who display a willingness to prioritize perceived health benefits over data protection concerns, which is a behavioral pattern that validates the privacy calculus framework. This calculated compromise is governed by a wide range of intersecting factors where privacy concern functions as a significant stop sign, while technical trust acts as a facilitator of data sharing. The more complex relationship between control preferences and data ownership beliefs further illustrates the multidimensional nature of user decision-making, where abstract principles of digital sovereignty interact with practical considerations of utility. These dynamics manifest heterogeneously across the population, with "Pragmatic Sharers," younger cohorts, and students exhibiting greater receptivity to health-benefit tradeoffs, while "Skeptics" and older demographics maintain stronger privacy boundaries. This empirically documented divergence backs up the thesis's central argument that current centralized models have normalized privacy concessions among regular users while simultaneously generating resistance among significant population segments with heightened privacy concerns, diminished trust, or stronger data ownership convictions. This pattern of selective acceptance based on perceived utility perfectly wraps up the privacy paradox, where users' theoretical concerns about privacy become subordinated to immediate, tangible benefits in actual decision-making contexts.
\subsection{Broader Implications of the Findings}
	% This section moves from direct answers to the wider impact of the research.
	\subsubsection{Theoretical Implications}
	% This subsubsection will discuss how the thesis contributes to academic theory. It will propose a refined model of user decision-making that separates privacy concern (a driver for caution in current systems) from control preference (a driver for adopting new systems). It will argue for the empirical validation of 'digital sovereignty' as a user-centric concept, not just a theoretical one.
	The logistic regression findings on willingness to share data refine theoretical models of user decision-making in data sharing contexts, establishing a conceptual distinction between deterrent and motivational factors. The results bakc up the theoretical starting point that data sharing within centralized systems operates primarily as a reactive mechanism driven by risk assessment rather than proactive agency seeking. The non-significance of control preference in predicting sharing behavior, despite its proven importance in adoption of decentralized solutions, provides theoretical validation for a forked model of user decision-making. One where privacy concerns function as powerful dealbreakers within existing systems, while control preferences operate as distinct motivational constructs that do not mitigate perceived risks in the usual playbook. This theoretical advancement, as we have seen, directly addresses the "privacy paradox" by demonstrating that users' abstract desires for control find no practical outlet in current "take it or leave it" scenarios, forcing them to default to decisions based purely on concern levels. The model thus reframes user decision-making not as a single-dimension privacy calculus, but as a multi-dimensional construct where the current centralized recipe fundamentally restricts users to risk-averse, defensive postures defined by what they fear losing rather than what they hope to gain through agency. This theoretical refinement explains why decentralized technologies, by explicitly shifting this calculus from risk mitigation to empowerment, represent an interesting transformation of the decision-making framework itself.

	The other regression analysis on willingness to adopt new technologies transforms theoretical models of user decision-making by validating a conceptual distinction between reactive and proactive motivational constructs. The non-significance of privacy concern in predicting adoption intentions, despite its central role in dealbreaking data sharing, provides further theoretical validation for a dual-process decision framework where qualitatively different psychological mechanisms govern defensive versus aspirational behaviors. This distinction moves theoretical discourse beyond the simplified "privacy paradox" toward a more sophisticated model where user motivation operates through parallel but independent processes. Risk assessment in existing systems versus agency pursuit in novel paradigms. The statistical significance of control preference and tech trust establishes that adoption is theoretically rooted in constructive, forward-looking motivations rather than reactive concerns. The empirical independence of normative data ownership beliefs from behavioral adoption intentions further challenges theoretical assumptions about the relationship between ethical positions and practical choices, suggesting that adoption theory must distinguish between abstract principles and actionable preferences. These findings establish the theoretical foundation for reconceptualizing user decision-making as an interplay between distinct motivational systems, providing a more nuanced explanatory framework that accounts for the apparent contradictions in how users evaluate and respond to different technological paradigms.

	The last regression analysis on trust in new technologies contributes itself as a distinctive theoretical framework for understanding trust formation in emerging data paradigms, revealing that trust operates as a dynamic equilibrium rather than a static sentiment. The model empirically validates a bidirectional trust construction theory, where privacy concern actively diminish trust through negative influence while control preference independently builds trust through positive influence, which is a conceptual divergence from traditional unidimensional trust models. This dual-pathway construct explains why privacy-protective features alone are insufficient for building user confidence. Trust in decentralized systems is not simply the absence of distrust but requires the affirmative presence of empowerment mechanisms. The finding that normative data ownership beliefs remain statistically insignificant in predicting trust challenges established theoretical assumptions about the relationship between abstract principles and trust formation, suggesting that ideological alignment does not automatically translate into confidence without tangible implementation. These results necessitate a theoretical pivot in how trust is conceptualized in technology adoption models. Rather than viewing trust as the dependent outcome of privacy protection, it emerges as an independent psychological construct actively shaped by perceived agency. This reconceptualization offers a theoretical bridge between the privacy paradox and adoption intention by positioning trust as the mediating mechanism through which abstract control preferences are converted into concrete behavioral intentions. The empirical equilibrium between negative privacy effects and positive control effects suggests that trust in new technologies exists at the nexus of fear mitigation and agency promotion, with neither alone sufficient to explain the complex psychology underlying user confidence in novel data paradigms.

	The empirical independence of data ownership belief from other core attitudinal constructs introduces a theoretical reconfiguration of how digital sovereignty is conceptualized. The statistical autonomy of this construct, manifesting as a strong blueprint-like position that exists independently of privacy concerns, control preferences, and technological trust, challenges current dominating theoretical frameworks that tend to collapse data ownership into broader privacy or agency discussions. This independence suggests that data ownership operates in its distinct axis dimension within users' cognitive architecture, representing an important established stance rather than an instrumental or reactive position. The theoretical implication of this finding is that digital sovereignty must be understood as a multi-layered construct where abstract ownership principles and concrete control mechanisms function as separate, though potentially complementary, domains. This conceptual separation explains previously observed contradictions in privacy behavior research, where strong ownership beliefs may coexist with out of sync sharing decisions or control preferences. Furthermore, the consistency of this independent construct across demographic and behavioral segments indicates that data ownership represents a established principle. Which is a theoretical finding that complicates purely instrumental or utilitarian models of user decision-making. This empirical validation of data ownership as a distinct theoretical construct necessitates more nuanced theoretical frameworks that can accommodate both the abstract, principle-based dimensions of digital sovereignty alongside its more pragmatic, control-oriented manifestations, thus advancing beyond simplified models that treat privacy, control, and ownership as interchangeable or necessarily correlated aspects of user agency.

	The factor analysis and structural equation modeling provide other validation for a theoretical reconceptualization of user attitudes in data governance contexts, revealing a tripartite structure that challenges unidimensional frameworks. The emergence of three distinct, largely orthogonal factors—(1) Openness to Decentralized Solutions, (2) Privacy Concern and Control Desire, and (3) Trust in Current Systems—empirically demonstrates that user perceptions operate as independent psychological constructs. This multidimensionality directly refines theoretical models by establishing that adoption intent is structurally separate from privacy anxiety, explaining why privacy-protective messaging alone fails to drive adoption of novel systems. The orthogonality between trust in decentralized models and trust in centralized ones breaks down the theoretical assumption of a zero-sum relationship, indicating that users can simultaneously maintain confidence in existing systems while exploring alternatives, a finding that transforms how adoption pathways should be conceptualized. The SEM's failure to detect significant causal relationships between these latent factors further shows their independence and challenges the theoretical framing of privacy calculus as a direct trade-off, suggesting instead that distinct psychological mechanisms govern defensive versus aspirational behaviors. The weak loading of data ownership belief across all factors, despite its strong presence as an established stance, theoretically separates abstract principles from actionable preferences and indicates that digital sovereignty requires translation from conceptual rights to tangible mechanisms. These structural findings collectively necessitate a theoretical pivot from monolithic user models toward segmented, multidimensional frameworks that can accommodate the empirical reality of fragmented and non-overlapping attitudinal constructs which provides a robust quantitative validation for rejecting one-size-fits-all theoretical approaches in favor of more nuanced, adaptive conceptualizations of user agency in data governance contexts.
	\subsubsection{Translating Research into Business Strategy}
	These findings offer a blueprint for companies developing decentralized data solutions in a market shaped by the tension between Dataism's efficiency and users' desire for agency. The segmentation results demand a fundamental rethinking of how products are designed and marketed, as one size clearly doesn't fit all when it comes to data governance. Managers face a strategic shift. They can either develop dual-interface systems or choose to specialize by targeting a specific user cluster. For those pursuing the former approach, adaptable platforms must be built that allow users to toggle between what we might call "Skeptic Mode" (emphasizing individual sovereignty with granular controls and minimal corporate involvement) and "Pragmatic Mode" (foregrounding institutional safeguards and ethical commitments). The strong beta coefficient for control preference ($\beta = 0.84$) sends a clear message to product teams. Interfaces must deliver tangible agency rather than abstract privacy promises. This requires a fundamental engineering pivot toward making user control visible, immediate, and meaningful at every interaction point.

	The market intelligence gathered on brand distribution patterns also presents managers with both strategic challenges and segmentation opportunities. Developers face an complicated dance with already existing ecosystems like Apple and Garmin who aren't just market leaders but trust proxies. This means that blockchain startups and decentralized solution providers must decide whether to position themselves as ecosystem players (through robust API integrations and partnerships) or principled alternatives (with distinct privacy-sovereignty value propositions). The most agile companies will likely employ both strategies simultaneously, developing modular architectures that can integrate with trusted ecosystems while maintaining core privacy principles. Moreover, the brand-attitude alignment revealed in the data suggests that positioning should explicitly address how solutions either complement or fundamentally reimagine data governance paradigms. Indeed, as we have seen, technical specifications alone won't drive adoption in a market where brand choices function as expressions of underlying privacy values. For maximum market penetration, managers should consider configurable governance options that allow users to calibrate their preferred balance of institutional and individual responsibility, thereby accommodating the full spectrum of user attitudes from defensive individualism to corporate stewardship
\subsection{Limitations and Future Research Directions}
	% This final section addresses the study's constraints and proposes new research avenues.
	\subsubsection{Limitations of the Current Study}
	This study comes with significant boundaries that shape how we should interpret the results. The research is exploratory rather than confirmatory. It identifies patterns but cannot establish cause-and-effect relationships. This matters. Statistical techniques like clustering revealed only modest differentiation between our user segments (silhouette score of just 0.20), indicating our "Pragmatic Sharers" and "Skeptics" are more like overlapping circles than distinct islands on the attitudinal map. The structural equation model, despite sophisticated bootstrapping, failed to detect significant relationships, leaving us with parallel rather than interconnected dimensions of user attitudes.
	
	Our sample represents another critical constraint. The sample is heavily dominated by Swiss (65.5\%) and Mexican (18.9\%) respondents, with young adults aged 25-34 making up over half the participants (52.9\%). The educational profile tilts sharply toward higher education (72.7\% with university degrees). This demographic imbalance means our findings are more like a spotlight on the population, illuminating specific segments rather than the entire wearable user population. Some groups like older adults (+65), doctoral degree holder and certain nationalities are represented by numbers too small for reliable statistical inference. The qualitative insights remain particularly thin, with only 11 respondents providing substantive comments and no follow-up interviews to deepen our understanding.
	
	Measurement issues represent the third challenge. A  blind spot exists in our understanding of incentives for adoption, token rewards, data monetization, value exchanges, as these questions were omitted from the survey. This leaves part of our research question on adoption drivers unanswered. Moreover, nearly half of respondents (48.3\%) were unfamiliar with decentralized technologies. How can someone evaluate what they don't understand? This unfamiliarity likely colored their responses, creating a knowledge gap that shapes our findings. The multi-language approach, while expanding our reach, potentially introduced inconsistencies despite careful translation. Technical concepts like blockchain and personal data vaults may have remained abstract notions rather than concrete possibilities for many participants.
	
	External validity constraints complete our limitations. Our comparisons with the 2023 Pew Research Center data, while valuable, face obstacles of different sampling approaches, question phrasings, and response scales. 
	Differences that limit direct equivalence. Our focus on emerging, largely unfamiliar decentralized technologies further complicates contextual relevance compared to studies examining more established systems.
	
	These limitations don't invalidate our findings but frame them properly as early explorations in a complex territory that requires further mapping. The results should be viewed as promising directions rather than definitive conclusions.
	\subsubsection{Recommendations for Future Research}
	% This subsubsection will propose specific future studies. It will suggest longitudinal research to track attitude changes, experimental studies to test different control-centric interfaces, and qualitative work to explore the nuances of the 'Skeptics' vs. 'Pragmatic Sharers' clusters. It will explicitly recommend future studies to investigate the role of incentives and the privacy paradox.

	The empirical findings of this thesis shows a complex landscape of user attitudes toward decentralized data management for wearables, but like any exploratory research, they raise as many questions as they answer. Building on our identified limitations, we propose a systematic research agenda that could deepen understanding and bridge the gap between theoretical promise and practical adoption.
	
	\paragraph{Longitudinal Studies to Track Evolving Attitudes}
	An interesting next step would be following users over time as they encounter and adapt to decentralized technologies. The cross-sectional snapshot we've captured reflects a moment when awareness remains limited—nearly half our sample had no familiarity with these systems. But attitudes aren't static. Longitudinal research tracking a panel of users over 2-3 years could reveal how trust, privacy concerns, and adoption willingness evolve as technologies diffuse through the population. This would help answer fascinating questions: Does the privacy paradox diminish with experience? Do our "Pragmatic Sharers" and "Skeptics" segments remain stable or converge? Such research would provide valuable insights into the temporal dimensions of user attitudes that our current data cannot address.
	
	\paragraph{Experimental Testing of Control-Centric Interfaces}
	Our finding that control preference serves as the primary driver of adoption willingness ($\beta=0.84$, $p<0.001$) deserves direct application in interface design research. The next logical step is prototyping and testing interfaces that operationalize control in different ways. Future studies should compare how users respond to various control mechanisms, so from granular permission settings to visual representations of data flows to blockchain verification displays. We could then compare adoption rates, trust formation, and satisfaction across interface variants. This would translate our theoretical findings into practical design guidelines. After all, what good is knowing control drives adoption if we can't deliver that sense of agency through tangible interface elements?
	
	\paragraph{Mixed-Methods Exploration of User Segments}
	The modest differentiation between our user segments (silhouette score=0.20) points to an opportunity for more nuanced qualitative research. Our "Pragmatic Sharers" and "Skeptics" not clearly distinct. Future research should employ in-depth interviews, focus groups, and observational methods to explore the decision-making processes that differentiate these groups. This is particularly important for understanding the "Skeptics" segment, who present an interesting paradox, where they express strong desire for control yet remain reluctant to adopt the very technologies designed to provide it. What mental models and trust barriers shape this apparent contradiction?
	
	\paragraph{Investigation of Incentive Structures}
	A blind spot in our current research, and a significant one, is how incentives influence adoption decisions. Future studies should systematically investigate how different value propositions affect willingness to adopt decentralized systems. What happens when users are offered token rewards? Data monetization options? Enhanced health insights? Interoperability benefits? Experimental designs presenting various incentive models could determine which most effectively overcome adoption barriers. This research would be particularly valuable for addressing aspects of the privacy paradox through properly aligned incentive structures. The gap between privacy concerns and actual behavior might narrow when users perceive tangible value in exchange for data sovereignty.
	
	\paragraph{Cross-Cultural Comparative Analysis}
	The notable differences we observed between language groups do not seem to be just statistical noise. They reflect deeper cultural variations in privacy expectations and trust formation. Future research should conduct balanced multinational studies examining how cultural factors and regulatory environments shape concepts of data ownership and trust. With representative samples across multiple countries with varying data protection frameworks (e.g., GDPR-governed regions versus less regulated markets), researchers could develop culturally sensitive implementations of decentralized technologies. Our Swiss-dominated sample (65.5\%) limits the generalizability of findings across cultural contexts that may have fundamentally different relationships with institutional trust and privacy.
	
	\paragraph{Methodological Improvements}
	Future research would benefit from addressing the methodological limitations we encountered. First, experimental designs should replace correlational approaches to establish causal relationships between attitudes and behaviors. Second, research should include educational components to ensure participants adequately understand decentralized technologies before evaluating them. Because you can't meaningfully assess what you don't understand. Third, standardized measures of control preference, privacy concern, and tech trust tailored to decentralized contexts should be developed, building on our composite measures to facilitate cross-study comparisons as this literature matures.
	
	The ultimate goal of this research agenda is bridging the gap between theoretical promise and practical adoption. Only via  continued empirical investigation can we determine whether decentralized systems will fulfill their potential as viable alternatives to the Dataist paradigm that currently dominates our digital interactions. The question isn't just academic. Indeed, it's whether users can reclaim meaningful agency over the data that increasingly defines and shapes

\section{Conclusion}
This thesis investigated the complex relationship between wearable device users and decentralized data management technologies against the increasing Dataistic ideology, where data flows are prioritized over individual agency. The results shows an interesting view. Users aren't a monolithic group but fall along a spectrum between "Pragmatic Sharers" and "Skeptics," each with distinct approaches to the privacy-utility tradeoff. Interestingly, there is a gap between perceived control (65\% report having little to none) and desired control (85\% want high or complete). This represents a failure of centralized data governance models.

An interesting contribution of this research is identifying that control preference, not privacy concern, drives willingness to adopt decentralized solutions. This finding reframes the privacy calculus. While privacy concerns act as dealbreakers within centralized systems, the promise of tangible control can channel those same concerns into constructive interest in alternatives. The path from concern to adoption isn't direct but requires the bridge of perceived agency.

However, we can't ignore the substantial barriers standing in the way. Nearly half of respondents remain unfamiliar with decentralized technologies, so you can't demand what you don't know exists. Security concerns (50.6\%), time burden (42.2\%), and complexity (40.9\%) further complicate adoption. The privacy paradox manifests clearly. Indeed, despite widespread concern, 67.6\% of users never read privacy policies. Abstract principles rarely translate into concrete behaviors without supporting mechanisms.

These findings have both theoretical and practical significance. Theoretically, they support a dual-process model of user decision-making where different psychological mechanisms govern defensive versus aspirational behaviors. Privacy concerns function as stop signs within existing systems, while control preferences operate as green lights for exploring alternatives. Practically, developers of decentralized technologies should prioritize visible, tangible control mechanisms over abstract privacy promises.

This research isn't without limitations. Our sample skews toward younger, highly educated Swiss respondents, and the exploratory nature identifies patterns but can't establish causal relationships. The modest differentiation between user segments (silhouette score 0.20) indicates overlapping rather than distinct categories. Moreover, widespread unfamiliarity with decentralized technologies means many evaluations were based on limited understanding.

The goal of this thesis was never to critique Dataism entirely. Data is indeed a powerful tool for understanding and improving our world, and algorithms may increasingly make better decisions than humans in many domains. But the challenge lies in ensuring this power respects individual rights and promotes collective well-being. Current data practices aren't the only possible approach and giving up control shouldn't be the default price of service. As we move forward, the question isn't simply whether decentralized technologies will become mainstream, but whether they can evolve to bridge the persistent gap between users' desire for control and their lived experience of digital disempowerment. The ongoing negotiation between algorithmic efficiency and human agency defines our technological era, and this research argues for solutions that enhance rather than dilutes the sovereignty of the individual.
